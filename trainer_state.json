{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.50682261208577,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015594541910331383,
      "grad_norm": 17.664066314697266,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.938,
      "step": 2
    },
    {
      "epoch": 0.031189083820662766,
      "grad_norm": 17.481138229370117,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.9434,
      "step": 4
    },
    {
      "epoch": 0.04678362573099415,
      "grad_norm": 15.876106262207031,
      "learning_rate": 6e-06,
      "loss": 2.1991,
      "step": 6
    },
    {
      "epoch": 0.06237816764132553,
      "grad_norm": 15.025181770324707,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9893,
      "step": 8
    },
    {
      "epoch": 0.07797270955165692,
      "grad_norm": 12.136540412902832,
      "learning_rate": 1e-05,
      "loss": 1.8637,
      "step": 10
    },
    {
      "epoch": 0.0935672514619883,
      "grad_norm": 14.226201057434082,
      "learning_rate": 1.2e-05,
      "loss": 1.9807,
      "step": 12
    },
    {
      "epoch": 0.10916179337231968,
      "grad_norm": 12.717979431152344,
      "learning_rate": 1.4e-05,
      "loss": 1.9776,
      "step": 14
    },
    {
      "epoch": 0.12475633528265107,
      "grad_norm": 13.225293159484863,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.9783,
      "step": 16
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 12.676193237304688,
      "learning_rate": 1.8e-05,
      "loss": 1.5605,
      "step": 18
    },
    {
      "epoch": 0.15594541910331383,
      "grad_norm": 10.449167251586914,
      "learning_rate": 2e-05,
      "loss": 1.488,
      "step": 20
    },
    {
      "epoch": 0.17153996101364521,
      "grad_norm": 12.683130264282227,
      "learning_rate": 1.9979797979797982e-05,
      "loss": 1.3524,
      "step": 22
    },
    {
      "epoch": 0.1871345029239766,
      "grad_norm": 13.910890579223633,
      "learning_rate": 1.995959595959596e-05,
      "loss": 1.4763,
      "step": 24
    },
    {
      "epoch": 0.20272904483430798,
      "grad_norm": 10.877778053283691,
      "learning_rate": 1.993939393939394e-05,
      "loss": 1.2604,
      "step": 26
    },
    {
      "epoch": 0.21832358674463936,
      "grad_norm": 11.171516418457031,
      "learning_rate": 1.991919191919192e-05,
      "loss": 1.348,
      "step": 28
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 9.404906272888184,
      "learning_rate": 1.98989898989899e-05,
      "loss": 1.2791,
      "step": 30
    },
    {
      "epoch": 0.24951267056530213,
      "grad_norm": 16.566343307495117,
      "learning_rate": 1.9878787878787878e-05,
      "loss": 1.1407,
      "step": 32
    },
    {
      "epoch": 0.2651072124756335,
      "grad_norm": 17.59694480895996,
      "learning_rate": 1.985858585858586e-05,
      "loss": 1.1047,
      "step": 34
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 22.07415199279785,
      "learning_rate": 1.983838383838384e-05,
      "loss": 1.1344,
      "step": 36
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 11.850820541381836,
      "learning_rate": 1.981818181818182e-05,
      "loss": 0.9998,
      "step": 38
    },
    {
      "epoch": 0.31189083820662766,
      "grad_norm": 15.44723129272461,
      "learning_rate": 1.97979797979798e-05,
      "loss": 1.0804,
      "step": 40
    },
    {
      "epoch": 0.32748538011695905,
      "grad_norm": 11.001923561096191,
      "learning_rate": 1.977777777777778e-05,
      "loss": 1.0274,
      "step": 42
    },
    {
      "epoch": 0.34307992202729043,
      "grad_norm": 11.113224983215332,
      "learning_rate": 1.975757575757576e-05,
      "loss": 1.0383,
      "step": 44
    },
    {
      "epoch": 0.3586744639376218,
      "grad_norm": 6.37322473526001,
      "learning_rate": 1.973737373737374e-05,
      "loss": 0.9219,
      "step": 46
    },
    {
      "epoch": 0.3742690058479532,
      "grad_norm": 12.182888984680176,
      "learning_rate": 1.971717171717172e-05,
      "loss": 0.9809,
      "step": 48
    },
    {
      "epoch": 0.3898635477582846,
      "grad_norm": 15.262284278869629,
      "learning_rate": 1.96969696969697e-05,
      "loss": 0.958,
      "step": 50
    },
    {
      "epoch": 0.40545808966861596,
      "grad_norm": 5.530254364013672,
      "learning_rate": 1.9676767676767677e-05,
      "loss": 0.8373,
      "step": 52
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 10.436663627624512,
      "learning_rate": 1.9656565656565658e-05,
      "loss": 0.9267,
      "step": 54
    },
    {
      "epoch": 0.43664717348927873,
      "grad_norm": 9.585639953613281,
      "learning_rate": 1.963636363636364e-05,
      "loss": 0.9243,
      "step": 56
    },
    {
      "epoch": 0.4522417153996101,
      "grad_norm": 8.790220260620117,
      "learning_rate": 1.961616161616162e-05,
      "loss": 0.8387,
      "step": 58
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 8.899242401123047,
      "learning_rate": 1.9595959595959596e-05,
      "loss": 0.8432,
      "step": 60
    },
    {
      "epoch": 0.4834307992202729,
      "grad_norm": 8.381714820861816,
      "learning_rate": 1.9575757575757577e-05,
      "loss": 0.787,
      "step": 62
    },
    {
      "epoch": 0.49902534113060426,
      "grad_norm": 10.286931991577148,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.7016,
      "step": 64
    },
    {
      "epoch": 0.5146198830409356,
      "grad_norm": 10.428186416625977,
      "learning_rate": 1.9535353535353534e-05,
      "loss": 0.8758,
      "step": 66
    },
    {
      "epoch": 0.530214424951267,
      "grad_norm": 9.998964309692383,
      "learning_rate": 1.9515151515151515e-05,
      "loss": 0.925,
      "step": 68
    },
    {
      "epoch": 0.5458089668615984,
      "grad_norm": 9.072585105895996,
      "learning_rate": 1.9494949494949496e-05,
      "loss": 0.6097,
      "step": 70
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 11.842967987060547,
      "learning_rate": 1.9474747474747476e-05,
      "loss": 0.5886,
      "step": 72
    },
    {
      "epoch": 0.5769980506822612,
      "grad_norm": 12.051878929138184,
      "learning_rate": 1.9454545454545457e-05,
      "loss": 0.7508,
      "step": 74
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 9.867587089538574,
      "learning_rate": 1.9434343434343437e-05,
      "loss": 0.6725,
      "step": 76
    },
    {
      "epoch": 0.6081871345029239,
      "grad_norm": 7.513052940368652,
      "learning_rate": 1.9414141414141418e-05,
      "loss": 0.7126,
      "step": 78
    },
    {
      "epoch": 0.6237816764132553,
      "grad_norm": 9.970551490783691,
      "learning_rate": 1.9393939393939395e-05,
      "loss": 0.7935,
      "step": 80
    },
    {
      "epoch": 0.6393762183235867,
      "grad_norm": 8.757028579711914,
      "learning_rate": 1.9373737373737376e-05,
      "loss": 0.6853,
      "step": 82
    },
    {
      "epoch": 0.6549707602339181,
      "grad_norm": 11.006183624267578,
      "learning_rate": 1.9353535353535356e-05,
      "loss": 0.6385,
      "step": 84
    },
    {
      "epoch": 0.6705653021442495,
      "grad_norm": 12.26672077178955,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.7089,
      "step": 86
    },
    {
      "epoch": 0.6861598440545809,
      "grad_norm": 12.864086151123047,
      "learning_rate": 1.9313131313131314e-05,
      "loss": 0.8167,
      "step": 88
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 8.52921199798584,
      "learning_rate": 1.9292929292929295e-05,
      "loss": 0.8501,
      "step": 90
    },
    {
      "epoch": 0.7173489278752436,
      "grad_norm": 9.021527290344238,
      "learning_rate": 1.9272727272727275e-05,
      "loss": 0.7648,
      "step": 92
    },
    {
      "epoch": 0.732943469785575,
      "grad_norm": 44.135066986083984,
      "learning_rate": 1.9252525252525252e-05,
      "loss": 0.8775,
      "step": 94
    },
    {
      "epoch": 0.7485380116959064,
      "grad_norm": 12.037755966186523,
      "learning_rate": 1.9232323232323233e-05,
      "loss": 0.9737,
      "step": 96
    },
    {
      "epoch": 0.7641325536062378,
      "grad_norm": 10.200048446655273,
      "learning_rate": 1.9212121212121213e-05,
      "loss": 0.6845,
      "step": 98
    },
    {
      "epoch": 0.7797270955165692,
      "grad_norm": 11.540081024169922,
      "learning_rate": 1.9191919191919194e-05,
      "loss": 0.6772,
      "step": 100
    },
    {
      "epoch": 0.7953216374269005,
      "grad_norm": 9.206451416015625,
      "learning_rate": 1.917171717171717e-05,
      "loss": 0.8881,
      "step": 102
    },
    {
      "epoch": 0.8109161793372319,
      "grad_norm": 8.895729064941406,
      "learning_rate": 1.9151515151515152e-05,
      "loss": 0.6524,
      "step": 104
    },
    {
      "epoch": 0.8265107212475633,
      "grad_norm": 9.216656684875488,
      "learning_rate": 1.9131313131313132e-05,
      "loss": 0.8926,
      "step": 106
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 12.845941543579102,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.7345,
      "step": 108
    },
    {
      "epoch": 0.8576998050682261,
      "grad_norm": 9.450898170471191,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 0.72,
      "step": 110
    },
    {
      "epoch": 0.8732943469785575,
      "grad_norm": 9.60248851776123,
      "learning_rate": 1.9070707070707074e-05,
      "loss": 0.8024,
      "step": 112
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 7.598193645477295,
      "learning_rate": 1.905050505050505e-05,
      "loss": 0.6775,
      "step": 114
    },
    {
      "epoch": 0.9044834307992202,
      "grad_norm": 9.479467391967773,
      "learning_rate": 1.9030303030303032e-05,
      "loss": 0.6947,
      "step": 116
    },
    {
      "epoch": 0.9200779727095516,
      "grad_norm": 10.128592491149902,
      "learning_rate": 1.9010101010101012e-05,
      "loss": 0.8423,
      "step": 118
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 10.57110595703125,
      "learning_rate": 1.8989898989898993e-05,
      "loss": 0.6967,
      "step": 120
    },
    {
      "epoch": 0.9512670565302144,
      "grad_norm": 7.178631782531738,
      "learning_rate": 1.896969696969697e-05,
      "loss": 0.7628,
      "step": 122
    },
    {
      "epoch": 0.9668615984405458,
      "grad_norm": 9.682772636413574,
      "learning_rate": 1.894949494949495e-05,
      "loss": 0.7368,
      "step": 124
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 9.833602905273438,
      "learning_rate": 1.892929292929293e-05,
      "loss": 0.7352,
      "step": 126
    },
    {
      "epoch": 0.9980506822612085,
      "grad_norm": 7.4460978507995605,
      "learning_rate": 1.8909090909090912e-05,
      "loss": 0.5639,
      "step": 128
    },
    {
      "epoch": 1.0077972709551657,
      "grad_norm": 9.586358070373535,
      "learning_rate": 1.888888888888889e-05,
      "loss": 0.3765,
      "step": 130
    },
    {
      "epoch": 1.023391812865497,
      "grad_norm": 10.073269844055176,
      "learning_rate": 1.886868686868687e-05,
      "loss": 0.6409,
      "step": 132
    },
    {
      "epoch": 1.0389863547758285,
      "grad_norm": 8.193013191223145,
      "learning_rate": 1.884848484848485e-05,
      "loss": 0.6176,
      "step": 134
    },
    {
      "epoch": 1.0545808966861598,
      "grad_norm": 11.545783042907715,
      "learning_rate": 1.8828282828282827e-05,
      "loss": 0.7685,
      "step": 136
    },
    {
      "epoch": 1.0701754385964912,
      "grad_norm": 9.503049850463867,
      "learning_rate": 1.8808080808080808e-05,
      "loss": 0.5631,
      "step": 138
    },
    {
      "epoch": 1.0857699805068226,
      "grad_norm": 10.386266708374023,
      "learning_rate": 1.8787878787878792e-05,
      "loss": 0.6752,
      "step": 140
    },
    {
      "epoch": 1.101364522417154,
      "grad_norm": 10.679681777954102,
      "learning_rate": 1.876767676767677e-05,
      "loss": 0.7265,
      "step": 142
    },
    {
      "epoch": 1.1169590643274854,
      "grad_norm": 11.775386810302734,
      "learning_rate": 1.874747474747475e-05,
      "loss": 0.7425,
      "step": 144
    },
    {
      "epoch": 1.1325536062378168,
      "grad_norm": 11.69855785369873,
      "learning_rate": 1.872727272727273e-05,
      "loss": 0.5866,
      "step": 146
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 10.612748146057129,
      "learning_rate": 1.8707070707070707e-05,
      "loss": 0.7917,
      "step": 148
    },
    {
      "epoch": 1.1637426900584795,
      "grad_norm": 10.507678031921387,
      "learning_rate": 1.8686868686868688e-05,
      "loss": 0.6044,
      "step": 150
    },
    {
      "epoch": 1.179337231968811,
      "grad_norm": 9.725646018981934,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.686,
      "step": 152
    },
    {
      "epoch": 1.1949317738791423,
      "grad_norm": 8.789236068725586,
      "learning_rate": 1.864646464646465e-05,
      "loss": 0.8457,
      "step": 154
    },
    {
      "epoch": 1.2105263157894737,
      "grad_norm": 10.90185832977295,
      "learning_rate": 1.8626262626262626e-05,
      "loss": 0.5814,
      "step": 156
    },
    {
      "epoch": 1.226120857699805,
      "grad_norm": 15.616350173950195,
      "learning_rate": 1.8606060606060607e-05,
      "loss": 0.8153,
      "step": 158
    },
    {
      "epoch": 1.2417153996101364,
      "grad_norm": 10.051962852478027,
      "learning_rate": 1.8585858585858588e-05,
      "loss": 0.8458,
      "step": 160
    },
    {
      "epoch": 1.2573099415204678,
      "grad_norm": 10.047449111938477,
      "learning_rate": 1.8565656565656568e-05,
      "loss": 0.5401,
      "step": 162
    },
    {
      "epoch": 1.2729044834307992,
      "grad_norm": 11.166009902954102,
      "learning_rate": 1.8545454545454545e-05,
      "loss": 0.6058,
      "step": 164
    },
    {
      "epoch": 1.2884990253411306,
      "grad_norm": 9.38355827331543,
      "learning_rate": 1.8525252525252526e-05,
      "loss": 0.5996,
      "step": 166
    },
    {
      "epoch": 1.304093567251462,
      "grad_norm": 9.961894035339355,
      "learning_rate": 1.8505050505050506e-05,
      "loss": 0.5721,
      "step": 168
    },
    {
      "epoch": 1.3196881091617934,
      "grad_norm": 10.401870727539062,
      "learning_rate": 1.8484848484848487e-05,
      "loss": 0.7657,
      "step": 170
    },
    {
      "epoch": 1.3352826510721247,
      "grad_norm": 10.837032318115234,
      "learning_rate": 1.8464646464646464e-05,
      "loss": 0.6323,
      "step": 172
    },
    {
      "epoch": 1.3508771929824561,
      "grad_norm": 8.006482124328613,
      "learning_rate": 1.8444444444444448e-05,
      "loss": 0.6295,
      "step": 174
    },
    {
      "epoch": 1.3664717348927875,
      "grad_norm": 8.512114524841309,
      "learning_rate": 1.8424242424242425e-05,
      "loss": 0.5683,
      "step": 176
    },
    {
      "epoch": 1.3820662768031189,
      "grad_norm": 8.704116821289062,
      "learning_rate": 1.8404040404040406e-05,
      "loss": 0.5993,
      "step": 178
    },
    {
      "epoch": 1.3976608187134503,
      "grad_norm": 9.18989372253418,
      "learning_rate": 1.8383838383838387e-05,
      "loss": 0.628,
      "step": 180
    },
    {
      "epoch": 1.4132553606237817,
      "grad_norm": 7.53916072845459,
      "learning_rate": 1.8363636363636367e-05,
      "loss": 0.7538,
      "step": 182
    },
    {
      "epoch": 1.428849902534113,
      "grad_norm": 9.04952621459961,
      "learning_rate": 1.8343434343434344e-05,
      "loss": 0.5394,
      "step": 184
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 9.215778350830078,
      "learning_rate": 1.8323232323232325e-05,
      "loss": 0.7355,
      "step": 186
    },
    {
      "epoch": 1.4600389863547758,
      "grad_norm": 9.932010650634766,
      "learning_rate": 1.8303030303030305e-05,
      "loss": 0.5399,
      "step": 188
    },
    {
      "epoch": 1.4756335282651072,
      "grad_norm": 12.015523910522461,
      "learning_rate": 1.8282828282828286e-05,
      "loss": 0.8292,
      "step": 190
    },
    {
      "epoch": 1.4912280701754386,
      "grad_norm": 8.187222480773926,
      "learning_rate": 1.8262626262626263e-05,
      "loss": 0.7226,
      "step": 192
    },
    {
      "epoch": 1.50682261208577,
      "grad_norm": 12.814352035522461,
      "learning_rate": 1.8242424242424244e-05,
      "loss": 0.5922,
      "step": 194
    },
    {
      "epoch": 1.5224171539961013,
      "grad_norm": 15.551886558532715,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.6715,
      "step": 196
    },
    {
      "epoch": 1.5380116959064327,
      "grad_norm": 10.843853950500488,
      "learning_rate": 1.82020202020202e-05,
      "loss": 0.6695,
      "step": 198
    },
    {
      "epoch": 1.553606237816764,
      "grad_norm": 11.853009223937988,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 0.8047,
      "step": 200
    },
    {
      "epoch": 1.5692007797270955,
      "grad_norm": 17.027219772338867,
      "learning_rate": 1.8161616161616163e-05,
      "loss": 0.7781,
      "step": 202
    },
    {
      "epoch": 1.5847953216374269,
      "grad_norm": 12.867636680603027,
      "learning_rate": 1.8141414141414143e-05,
      "loss": 0.6367,
      "step": 204
    },
    {
      "epoch": 1.6003898635477583,
      "grad_norm": 9.882416725158691,
      "learning_rate": 1.812121212121212e-05,
      "loss": 0.5129,
      "step": 206
    },
    {
      "epoch": 1.6159844054580896,
      "grad_norm": 10.113730430603027,
      "learning_rate": 1.8101010101010104e-05,
      "loss": 0.4394,
      "step": 208
    },
    {
      "epoch": 1.631578947368421,
      "grad_norm": 11.29802417755127,
      "learning_rate": 1.8080808080808085e-05,
      "loss": 0.7742,
      "step": 210
    },
    {
      "epoch": 1.6471734892787524,
      "grad_norm": 18.187814712524414,
      "learning_rate": 1.8060606060606062e-05,
      "loss": 0.5546,
      "step": 212
    },
    {
      "epoch": 1.6627680311890838,
      "grad_norm": 9.656949996948242,
      "learning_rate": 1.8040404040404043e-05,
      "loss": 0.9058,
      "step": 214
    },
    {
      "epoch": 1.6783625730994152,
      "grad_norm": 10.865768432617188,
      "learning_rate": 1.8020202020202023e-05,
      "loss": 0.6663,
      "step": 216
    },
    {
      "epoch": 1.6939571150097466,
      "grad_norm": 11.690841674804688,
      "learning_rate": 1.8e-05,
      "loss": 0.6705,
      "step": 218
    },
    {
      "epoch": 1.709551656920078,
      "grad_norm": 14.304877281188965,
      "learning_rate": 1.797979797979798e-05,
      "loss": 0.704,
      "step": 220
    },
    {
      "epoch": 1.7251461988304093,
      "grad_norm": 10.7765474319458,
      "learning_rate": 1.795959595959596e-05,
      "loss": 0.6727,
      "step": 222
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 12.268595695495605,
      "learning_rate": 1.7939393939393942e-05,
      "loss": 0.6711,
      "step": 224
    },
    {
      "epoch": 1.756335282651072,
      "grad_norm": 10.81279468536377,
      "learning_rate": 1.791919191919192e-05,
      "loss": 0.6711,
      "step": 226
    },
    {
      "epoch": 1.7719298245614035,
      "grad_norm": 10.919468879699707,
      "learning_rate": 1.78989898989899e-05,
      "loss": 0.431,
      "step": 228
    },
    {
      "epoch": 1.7875243664717348,
      "grad_norm": 16.300750732421875,
      "learning_rate": 1.787878787878788e-05,
      "loss": 0.7267,
      "step": 230
    },
    {
      "epoch": 1.8031189083820662,
      "grad_norm": 11.41988468170166,
      "learning_rate": 1.785858585858586e-05,
      "loss": 0.6634,
      "step": 232
    },
    {
      "epoch": 1.8187134502923976,
      "grad_norm": 8.900544166564941,
      "learning_rate": 1.7838383838383838e-05,
      "loss": 0.4348,
      "step": 234
    },
    {
      "epoch": 1.834307992202729,
      "grad_norm": 10.577929496765137,
      "learning_rate": 1.781818181818182e-05,
      "loss": 0.8053,
      "step": 236
    },
    {
      "epoch": 1.8499025341130604,
      "grad_norm": 12.520196914672852,
      "learning_rate": 1.77979797979798e-05,
      "loss": 0.6285,
      "step": 238
    },
    {
      "epoch": 1.8654970760233918,
      "grad_norm": 10.578001022338867,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.6465,
      "step": 240
    },
    {
      "epoch": 1.8810916179337231,
      "grad_norm": 18.903324127197266,
      "learning_rate": 1.775757575757576e-05,
      "loss": 0.6626,
      "step": 242
    },
    {
      "epoch": 1.8966861598440545,
      "grad_norm": 9.642838478088379,
      "learning_rate": 1.773737373737374e-05,
      "loss": 0.6028,
      "step": 244
    },
    {
      "epoch": 1.912280701754386,
      "grad_norm": 10.103790283203125,
      "learning_rate": 1.771717171717172e-05,
      "loss": 0.5388,
      "step": 246
    },
    {
      "epoch": 1.9278752436647173,
      "grad_norm": 37.49832534790039,
      "learning_rate": 1.76969696969697e-05,
      "loss": 0.6995,
      "step": 248
    },
    {
      "epoch": 1.9434697855750487,
      "grad_norm": 9.236486434936523,
      "learning_rate": 1.767676767676768e-05,
      "loss": 0.5665,
      "step": 250
    },
    {
      "epoch": 1.95906432748538,
      "grad_norm": 9.694302558898926,
      "learning_rate": 1.765656565656566e-05,
      "loss": 0.6461,
      "step": 252
    },
    {
      "epoch": 1.9746588693957114,
      "grad_norm": 10.21003246307373,
      "learning_rate": 1.7636363636363637e-05,
      "loss": 0.7483,
      "step": 254
    },
    {
      "epoch": 1.9902534113060428,
      "grad_norm": 9.510974884033203,
      "learning_rate": 1.7616161616161618e-05,
      "loss": 0.597,
      "step": 256
    },
    {
      "epoch": 2.0,
      "grad_norm": 6.268904209136963,
      "learning_rate": 1.75959595959596e-05,
      "loss": 0.4193,
      "step": 258
    },
    {
      "epoch": 2.0155945419103314,
      "grad_norm": 9.48487377166748,
      "learning_rate": 1.7575757575757576e-05,
      "loss": 0.528,
      "step": 260
    },
    {
      "epoch": 2.0311890838206628,
      "grad_norm": 8.562446594238281,
      "learning_rate": 1.7555555555555556e-05,
      "loss": 0.587,
      "step": 262
    },
    {
      "epoch": 2.046783625730994,
      "grad_norm": 10.65334415435791,
      "learning_rate": 1.7535353535353537e-05,
      "loss": 0.5616,
      "step": 264
    },
    {
      "epoch": 2.0623781676413255,
      "grad_norm": 9.189057350158691,
      "learning_rate": 1.7515151515151517e-05,
      "loss": 0.5517,
      "step": 266
    },
    {
      "epoch": 2.077972709551657,
      "grad_norm": 12.916790008544922,
      "learning_rate": 1.7494949494949494e-05,
      "loss": 0.7175,
      "step": 268
    },
    {
      "epoch": 2.0935672514619883,
      "grad_norm": 10.174715042114258,
      "learning_rate": 1.7474747474747475e-05,
      "loss": 0.532,
      "step": 270
    },
    {
      "epoch": 2.1091617933723197,
      "grad_norm": 11.584918975830078,
      "learning_rate": 1.7454545454545456e-05,
      "loss": 0.5064,
      "step": 272
    },
    {
      "epoch": 2.124756335282651,
      "grad_norm": 11.714339256286621,
      "learning_rate": 1.7434343434343436e-05,
      "loss": 0.641,
      "step": 274
    },
    {
      "epoch": 2.1403508771929824,
      "grad_norm": 10.85686206817627,
      "learning_rate": 1.7414141414141413e-05,
      "loss": 0.5256,
      "step": 276
    },
    {
      "epoch": 2.155945419103314,
      "grad_norm": 10.583471298217773,
      "learning_rate": 1.7393939393939397e-05,
      "loss": 0.5273,
      "step": 278
    },
    {
      "epoch": 2.171539961013645,
      "grad_norm": 11.772512435913086,
      "learning_rate": 1.7373737373737375e-05,
      "loss": 0.6417,
      "step": 280
    },
    {
      "epoch": 2.1871345029239766,
      "grad_norm": 14.329418182373047,
      "learning_rate": 1.7353535353535355e-05,
      "loss": 0.5499,
      "step": 282
    },
    {
      "epoch": 2.202729044834308,
      "grad_norm": 10.103382110595703,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.6625,
      "step": 284
    },
    {
      "epoch": 2.2183235867446394,
      "grad_norm": 13.004687309265137,
      "learning_rate": 1.7313131313131316e-05,
      "loss": 0.5178,
      "step": 286
    },
    {
      "epoch": 2.2339181286549707,
      "grad_norm": 13.438318252563477,
      "learning_rate": 1.7292929292929293e-05,
      "loss": 0.6256,
      "step": 288
    },
    {
      "epoch": 2.249512670565302,
      "grad_norm": 10.062450408935547,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 0.5178,
      "step": 290
    },
    {
      "epoch": 2.2651072124756335,
      "grad_norm": 13.006471633911133,
      "learning_rate": 1.7252525252525255e-05,
      "loss": 0.5393,
      "step": 292
    },
    {
      "epoch": 2.280701754385965,
      "grad_norm": 6.5147576332092285,
      "learning_rate": 1.7232323232323235e-05,
      "loss": 0.4998,
      "step": 294
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 12.856680870056152,
      "learning_rate": 1.7212121212121212e-05,
      "loss": 0.5659,
      "step": 296
    },
    {
      "epoch": 2.3118908382066277,
      "grad_norm": 10.986842155456543,
      "learning_rate": 1.7191919191919193e-05,
      "loss": 0.6219,
      "step": 298
    },
    {
      "epoch": 2.327485380116959,
      "grad_norm": 15.712268829345703,
      "learning_rate": 1.7171717171717173e-05,
      "loss": 0.8307,
      "step": 300
    },
    {
      "epoch": 2.3430799220272904,
      "grad_norm": 11.574236869812012,
      "learning_rate": 1.715151515151515e-05,
      "loss": 0.5302,
      "step": 302
    },
    {
      "epoch": 2.358674463937622,
      "grad_norm": 10.205121994018555,
      "learning_rate": 1.713131313131313e-05,
      "loss": 0.5464,
      "step": 304
    },
    {
      "epoch": 2.374269005847953,
      "grad_norm": 9.648726463317871,
      "learning_rate": 1.7111111111111112e-05,
      "loss": 0.4808,
      "step": 306
    },
    {
      "epoch": 2.3898635477582846,
      "grad_norm": 9.879901885986328,
      "learning_rate": 1.7090909090909092e-05,
      "loss": 0.6754,
      "step": 308
    },
    {
      "epoch": 2.405458089668616,
      "grad_norm": 13.033337593078613,
      "learning_rate": 1.707070707070707e-05,
      "loss": 0.6043,
      "step": 310
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 12.107264518737793,
      "learning_rate": 1.7050505050505054e-05,
      "loss": 0.4889,
      "step": 312
    },
    {
      "epoch": 2.4366471734892787,
      "grad_norm": 15.074712753295898,
      "learning_rate": 1.7030303030303034e-05,
      "loss": 0.6708,
      "step": 314
    },
    {
      "epoch": 2.45224171539961,
      "grad_norm": 13.332374572753906,
      "learning_rate": 1.701010101010101e-05,
      "loss": 0.6129,
      "step": 316
    },
    {
      "epoch": 2.4678362573099415,
      "grad_norm": 13.809693336486816,
      "learning_rate": 1.6989898989898992e-05,
      "loss": 0.7573,
      "step": 318
    },
    {
      "epoch": 2.483430799220273,
      "grad_norm": 15.963423728942871,
      "learning_rate": 1.6969696969696972e-05,
      "loss": 0.681,
      "step": 320
    },
    {
      "epoch": 2.4990253411306043,
      "grad_norm": 15.47554874420166,
      "learning_rate": 1.694949494949495e-05,
      "loss": 0.5266,
      "step": 322
    },
    {
      "epoch": 2.5146198830409356,
      "grad_norm": 11.654376983642578,
      "learning_rate": 1.692929292929293e-05,
      "loss": 0.4096,
      "step": 324
    },
    {
      "epoch": 2.530214424951267,
      "grad_norm": 11.49431037902832,
      "learning_rate": 1.690909090909091e-05,
      "loss": 0.5518,
      "step": 326
    },
    {
      "epoch": 2.5458089668615984,
      "grad_norm": 15.95789909362793,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.5413,
      "step": 328
    },
    {
      "epoch": 2.56140350877193,
      "grad_norm": 13.203900337219238,
      "learning_rate": 1.686868686868687e-05,
      "loss": 0.7396,
      "step": 330
    },
    {
      "epoch": 2.576998050682261,
      "grad_norm": 14.860455513000488,
      "learning_rate": 1.684848484848485e-05,
      "loss": 0.5641,
      "step": 332
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 14.773088455200195,
      "learning_rate": 1.682828282828283e-05,
      "loss": 0.5155,
      "step": 334
    },
    {
      "epoch": 2.608187134502924,
      "grad_norm": 14.796468734741211,
      "learning_rate": 1.680808080808081e-05,
      "loss": 0.6037,
      "step": 336
    },
    {
      "epoch": 2.6237816764132553,
      "grad_norm": 11.995277404785156,
      "learning_rate": 1.6787878787878787e-05,
      "loss": 0.4022,
      "step": 338
    },
    {
      "epoch": 2.6393762183235867,
      "grad_norm": 13.07720947265625,
      "learning_rate": 1.6767676767676768e-05,
      "loss": 0.6879,
      "step": 340
    },
    {
      "epoch": 2.654970760233918,
      "grad_norm": 11.444893836975098,
      "learning_rate": 1.674747474747475e-05,
      "loss": 0.5692,
      "step": 342
    },
    {
      "epoch": 2.6705653021442495,
      "grad_norm": 12.270146369934082,
      "learning_rate": 1.672727272727273e-05,
      "loss": 0.6429,
      "step": 344
    },
    {
      "epoch": 2.686159844054581,
      "grad_norm": 13.096720695495605,
      "learning_rate": 1.670707070707071e-05,
      "loss": 0.4978,
      "step": 346
    },
    {
      "epoch": 2.7017543859649122,
      "grad_norm": 9.279082298278809,
      "learning_rate": 1.668686868686869e-05,
      "loss": 0.6408,
      "step": 348
    },
    {
      "epoch": 2.7173489278752436,
      "grad_norm": 11.412774085998535,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.574,
      "step": 350
    },
    {
      "epoch": 2.732943469785575,
      "grad_norm": 14.664793014526367,
      "learning_rate": 1.6646464646464648e-05,
      "loss": 0.7706,
      "step": 352
    },
    {
      "epoch": 2.7485380116959064,
      "grad_norm": 11.174318313598633,
      "learning_rate": 1.662626262626263e-05,
      "loss": 0.3587,
      "step": 354
    },
    {
      "epoch": 2.7641325536062378,
      "grad_norm": 14.860897064208984,
      "learning_rate": 1.660606060606061e-05,
      "loss": 0.6155,
      "step": 356
    },
    {
      "epoch": 2.779727095516569,
      "grad_norm": 11.829504013061523,
      "learning_rate": 1.6585858585858586e-05,
      "loss": 0.4796,
      "step": 358
    },
    {
      "epoch": 2.7953216374269005,
      "grad_norm": 14.598243713378906,
      "learning_rate": 1.6565656565656567e-05,
      "loss": 0.4901,
      "step": 360
    },
    {
      "epoch": 2.810916179337232,
      "grad_norm": 13.160191535949707,
      "learning_rate": 1.6545454545454548e-05,
      "loss": 0.6734,
      "step": 362
    },
    {
      "epoch": 2.8265107212475633,
      "grad_norm": 13.110061645507812,
      "learning_rate": 1.6525252525252528e-05,
      "loss": 0.3767,
      "step": 364
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 17.741859436035156,
      "learning_rate": 1.6505050505050505e-05,
      "loss": 0.5395,
      "step": 366
    },
    {
      "epoch": 2.857699805068226,
      "grad_norm": 14.718367576599121,
      "learning_rate": 1.6484848484848486e-05,
      "loss": 0.6701,
      "step": 368
    },
    {
      "epoch": 2.8732943469785575,
      "grad_norm": 11.817237854003906,
      "learning_rate": 1.6464646464646466e-05,
      "loss": 0.4672,
      "step": 370
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 12.582974433898926,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 0.5072,
      "step": 372
    },
    {
      "epoch": 2.9044834307992202,
      "grad_norm": 15.838509559631348,
      "learning_rate": 1.6424242424242424e-05,
      "loss": 0.5617,
      "step": 374
    },
    {
      "epoch": 2.9200779727095516,
      "grad_norm": 13.270606994628906,
      "learning_rate": 1.6404040404040405e-05,
      "loss": 0.4268,
      "step": 376
    },
    {
      "epoch": 2.935672514619883,
      "grad_norm": 15.534038543701172,
      "learning_rate": 1.6383838383838385e-05,
      "loss": 0.5331,
      "step": 378
    },
    {
      "epoch": 2.9512670565302144,
      "grad_norm": 16.258068084716797,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 0.4773,
      "step": 380
    },
    {
      "epoch": 2.9668615984405458,
      "grad_norm": 12.097797393798828,
      "learning_rate": 1.6343434343434346e-05,
      "loss": 0.4536,
      "step": 382
    },
    {
      "epoch": 2.982456140350877,
      "grad_norm": 14.183737754821777,
      "learning_rate": 1.6323232323232324e-05,
      "loss": 0.5178,
      "step": 384
    },
    {
      "epoch": 2.9980506822612085,
      "grad_norm": 13.790335655212402,
      "learning_rate": 1.6303030303030304e-05,
      "loss": 0.6398,
      "step": 386
    },
    {
      "epoch": 3.007797270955166,
      "grad_norm": 13.845024108886719,
      "learning_rate": 1.6282828282828285e-05,
      "loss": 0.248,
      "step": 388
    },
    {
      "epoch": 3.023391812865497,
      "grad_norm": 12.816067695617676,
      "learning_rate": 1.6262626262626265e-05,
      "loss": 0.4594,
      "step": 390
    },
    {
      "epoch": 3.0389863547758287,
      "grad_norm": 12.812300682067871,
      "learning_rate": 1.6242424242424243e-05,
      "loss": 0.5117,
      "step": 392
    },
    {
      "epoch": 3.0545808966861596,
      "grad_norm": 13.531980514526367,
      "learning_rate": 1.6222222222222223e-05,
      "loss": 0.6217,
      "step": 394
    },
    {
      "epoch": 3.0701754385964914,
      "grad_norm": 14.7723388671875,
      "learning_rate": 1.6202020202020204e-05,
      "loss": 0.4795,
      "step": 396
    },
    {
      "epoch": 3.0857699805068224,
      "grad_norm": 14.345706939697266,
      "learning_rate": 1.6181818181818184e-05,
      "loss": 0.3947,
      "step": 398
    },
    {
      "epoch": 3.101364522417154,
      "grad_norm": 10.773722648620605,
      "learning_rate": 1.616161616161616e-05,
      "loss": 0.3788,
      "step": 400
    },
    {
      "epoch": 3.116959064327485,
      "grad_norm": 18.359590530395508,
      "learning_rate": 1.6141414141414142e-05,
      "loss": 0.4127,
      "step": 402
    },
    {
      "epoch": 3.132553606237817,
      "grad_norm": 18.70104217529297,
      "learning_rate": 1.6121212121212123e-05,
      "loss": 0.5222,
      "step": 404
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 12.833662986755371,
      "learning_rate": 1.6101010101010103e-05,
      "loss": 0.5968,
      "step": 406
    },
    {
      "epoch": 3.1637426900584797,
      "grad_norm": 15.819488525390625,
      "learning_rate": 1.608080808080808e-05,
      "loss": 0.5706,
      "step": 408
    },
    {
      "epoch": 3.1793372319688107,
      "grad_norm": 16.969459533691406,
      "learning_rate": 1.606060606060606e-05,
      "loss": 0.4471,
      "step": 410
    },
    {
      "epoch": 3.1949317738791425,
      "grad_norm": 14.440665245056152,
      "learning_rate": 1.604040404040404e-05,
      "loss": 0.5291,
      "step": 412
    },
    {
      "epoch": 3.2105263157894735,
      "grad_norm": 11.27415657043457,
      "learning_rate": 1.6020202020202022e-05,
      "loss": 0.4915,
      "step": 414
    },
    {
      "epoch": 3.2261208576998053,
      "grad_norm": 13.606315612792969,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.4372,
      "step": 416
    },
    {
      "epoch": 3.241715399610136,
      "grad_norm": 13.975041389465332,
      "learning_rate": 1.5979797979797983e-05,
      "loss": 0.6737,
      "step": 418
    },
    {
      "epoch": 3.257309941520468,
      "grad_norm": 24.185564041137695,
      "learning_rate": 1.595959595959596e-05,
      "loss": 0.5826,
      "step": 420
    },
    {
      "epoch": 3.272904483430799,
      "grad_norm": 14.552754402160645,
      "learning_rate": 1.593939393939394e-05,
      "loss": 0.5231,
      "step": 422
    },
    {
      "epoch": 3.288499025341131,
      "grad_norm": 16.747800827026367,
      "learning_rate": 1.591919191919192e-05,
      "loss": 0.4355,
      "step": 424
    },
    {
      "epoch": 3.3040935672514617,
      "grad_norm": 12.791067123413086,
      "learning_rate": 1.5898989898989902e-05,
      "loss": 0.4576,
      "step": 426
    },
    {
      "epoch": 3.3196881091617936,
      "grad_norm": 10.08392333984375,
      "learning_rate": 1.587878787878788e-05,
      "loss": 0.3891,
      "step": 428
    },
    {
      "epoch": 3.3352826510721245,
      "grad_norm": 16.392961502075195,
      "learning_rate": 1.585858585858586e-05,
      "loss": 0.3535,
      "step": 430
    },
    {
      "epoch": 3.3508771929824563,
      "grad_norm": 16.756135940551758,
      "learning_rate": 1.583838383838384e-05,
      "loss": 0.4458,
      "step": 432
    },
    {
      "epoch": 3.3664717348927873,
      "grad_norm": 15.207284927368164,
      "learning_rate": 1.5818181818181818e-05,
      "loss": 0.3945,
      "step": 434
    },
    {
      "epoch": 3.382066276803119,
      "grad_norm": 15.10335922241211,
      "learning_rate": 1.5797979797979798e-05,
      "loss": 0.5166,
      "step": 436
    },
    {
      "epoch": 3.39766081871345,
      "grad_norm": 20.15766143798828,
      "learning_rate": 1.577777777777778e-05,
      "loss": 0.5022,
      "step": 438
    },
    {
      "epoch": 3.413255360623782,
      "grad_norm": 18.510807037353516,
      "learning_rate": 1.575757575757576e-05,
      "loss": 0.4425,
      "step": 440
    },
    {
      "epoch": 3.428849902534113,
      "grad_norm": 18.193065643310547,
      "learning_rate": 1.5737373737373737e-05,
      "loss": 0.3623,
      "step": 442
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 18.21416473388672,
      "learning_rate": 1.5717171717171717e-05,
      "loss": 0.5746,
      "step": 444
    },
    {
      "epoch": 3.4600389863547756,
      "grad_norm": 13.079615592956543,
      "learning_rate": 1.5696969696969698e-05,
      "loss": 0.5341,
      "step": 446
    },
    {
      "epoch": 3.4756335282651074,
      "grad_norm": 19.243452072143555,
      "learning_rate": 1.5676767676767678e-05,
      "loss": 0.5462,
      "step": 448
    },
    {
      "epoch": 3.4912280701754383,
      "grad_norm": 17.762601852416992,
      "learning_rate": 1.565656565656566e-05,
      "loss": 0.4402,
      "step": 450
    },
    {
      "epoch": 3.50682261208577,
      "grad_norm": 16.528440475463867,
      "learning_rate": 1.563636363636364e-05,
      "loss": 0.4195,
      "step": 452
    },
    {
      "epoch": 3.522417153996101,
      "grad_norm": 19.318361282348633,
      "learning_rate": 1.5616161616161617e-05,
      "loss": 0.6058,
      "step": 454
    },
    {
      "epoch": 3.538011695906433,
      "grad_norm": 15.046871185302734,
      "learning_rate": 1.5595959595959597e-05,
      "loss": 0.4085,
      "step": 456
    },
    {
      "epoch": 3.553606237816764,
      "grad_norm": 18.950855255126953,
      "learning_rate": 1.5575757575757578e-05,
      "loss": 0.4929,
      "step": 458
    },
    {
      "epoch": 3.5692007797270957,
      "grad_norm": 16.322355270385742,
      "learning_rate": 1.555555555555556e-05,
      "loss": 0.521,
      "step": 460
    },
    {
      "epoch": 3.5847953216374266,
      "grad_norm": 13.848784446716309,
      "learning_rate": 1.5535353535353536e-05,
      "loss": 0.3852,
      "step": 462
    },
    {
      "epoch": 3.6003898635477585,
      "grad_norm": 19.18058204650879,
      "learning_rate": 1.5515151515151516e-05,
      "loss": 0.7287,
      "step": 464
    },
    {
      "epoch": 3.6159844054580894,
      "grad_norm": 20.523662567138672,
      "learning_rate": 1.5494949494949497e-05,
      "loss": 0.5009,
      "step": 466
    },
    {
      "epoch": 3.6315789473684212,
      "grad_norm": 16.41554832458496,
      "learning_rate": 1.5474747474747477e-05,
      "loss": 0.5487,
      "step": 468
    },
    {
      "epoch": 3.647173489278752,
      "grad_norm": 14.819306373596191,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 0.5088,
      "step": 470
    },
    {
      "epoch": 3.662768031189084,
      "grad_norm": 14.908061027526855,
      "learning_rate": 1.5434343434343435e-05,
      "loss": 0.5587,
      "step": 472
    },
    {
      "epoch": 3.678362573099415,
      "grad_norm": 18.91082763671875,
      "learning_rate": 1.5414141414141416e-05,
      "loss": 0.5701,
      "step": 474
    },
    {
      "epoch": 3.6939571150097468,
      "grad_norm": 18.614938735961914,
      "learning_rate": 1.5393939393939393e-05,
      "loss": 0.4255,
      "step": 476
    },
    {
      "epoch": 3.7095516569200777,
      "grad_norm": 14.473241806030273,
      "learning_rate": 1.5373737373737373e-05,
      "loss": 0.5381,
      "step": 478
    },
    {
      "epoch": 3.7251461988304095,
      "grad_norm": 18.88397789001465,
      "learning_rate": 1.5353535353535354e-05,
      "loss": 0.3638,
      "step": 480
    },
    {
      "epoch": 3.7407407407407405,
      "grad_norm": 14.756614685058594,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.4363,
      "step": 482
    },
    {
      "epoch": 3.7563352826510723,
      "grad_norm": 15.085126876831055,
      "learning_rate": 1.5313131313131315e-05,
      "loss": 0.4806,
      "step": 484
    },
    {
      "epoch": 3.7719298245614032,
      "grad_norm": 14.573981285095215,
      "learning_rate": 1.5292929292929296e-05,
      "loss": 0.4326,
      "step": 486
    },
    {
      "epoch": 3.787524366471735,
      "grad_norm": 16.576663970947266,
      "learning_rate": 1.5272727272727276e-05,
      "loss": 0.4203,
      "step": 488
    },
    {
      "epoch": 3.803118908382066,
      "grad_norm": 14.48440933227539,
      "learning_rate": 1.5252525252525255e-05,
      "loss": 0.464,
      "step": 490
    },
    {
      "epoch": 3.818713450292398,
      "grad_norm": 14.99740982055664,
      "learning_rate": 1.5232323232323234e-05,
      "loss": 0.3632,
      "step": 492
    },
    {
      "epoch": 3.8343079922027288,
      "grad_norm": 18.85546112060547,
      "learning_rate": 1.5212121212121213e-05,
      "loss": 0.4538,
      "step": 494
    },
    {
      "epoch": 3.8499025341130606,
      "grad_norm": 15.193532943725586,
      "learning_rate": 1.5191919191919193e-05,
      "loss": 0.401,
      "step": 496
    },
    {
      "epoch": 3.8654970760233915,
      "grad_norm": 15.932311058044434,
      "learning_rate": 1.5171717171717172e-05,
      "loss": 0.5357,
      "step": 498
    },
    {
      "epoch": 3.8810916179337234,
      "grad_norm": 14.84028434753418,
      "learning_rate": 1.5151515151515153e-05,
      "loss": 0.5792,
      "step": 500
    },
    {
      "epoch": 3.8966861598440543,
      "grad_norm": 17.088499069213867,
      "learning_rate": 1.5131313131313132e-05,
      "loss": 0.391,
      "step": 502
    },
    {
      "epoch": 3.912280701754386,
      "grad_norm": 14.664362907409668,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 0.6179,
      "step": 504
    },
    {
      "epoch": 3.927875243664717,
      "grad_norm": 14.838401794433594,
      "learning_rate": 1.5090909090909091e-05,
      "loss": 0.3554,
      "step": 506
    },
    {
      "epoch": 3.943469785575049,
      "grad_norm": 13.626789093017578,
      "learning_rate": 1.5070707070707072e-05,
      "loss": 0.3894,
      "step": 508
    },
    {
      "epoch": 3.95906432748538,
      "grad_norm": 19.34377670288086,
      "learning_rate": 1.505050505050505e-05,
      "loss": 0.582,
      "step": 510
    },
    {
      "epoch": 3.9746588693957117,
      "grad_norm": 15.020448684692383,
      "learning_rate": 1.5030303030303031e-05,
      "loss": 0.5696,
      "step": 512
    },
    {
      "epoch": 3.9902534113060426,
      "grad_norm": 19.702302932739258,
      "learning_rate": 1.501010101010101e-05,
      "loss": 0.4746,
      "step": 514
    },
    {
      "epoch": 4.0,
      "grad_norm": 8.575618743896484,
      "learning_rate": 1.4989898989898992e-05,
      "loss": 0.1863,
      "step": 516
    },
    {
      "epoch": 4.015594541910332,
      "grad_norm": 13.7453031539917,
      "learning_rate": 1.4969696969696971e-05,
      "loss": 0.5075,
      "step": 518
    },
    {
      "epoch": 4.031189083820663,
      "grad_norm": 16.493406295776367,
      "learning_rate": 1.4949494949494952e-05,
      "loss": 0.3138,
      "step": 520
    },
    {
      "epoch": 4.046783625730994,
      "grad_norm": 14.007159233093262,
      "learning_rate": 1.492929292929293e-05,
      "loss": 0.4174,
      "step": 522
    },
    {
      "epoch": 4.0623781676413255,
      "grad_norm": 14.07163143157959,
      "learning_rate": 1.4909090909090911e-05,
      "loss": 0.4399,
      "step": 524
    },
    {
      "epoch": 4.077972709551657,
      "grad_norm": 16.076597213745117,
      "learning_rate": 1.488888888888889e-05,
      "loss": 0.4321,
      "step": 526
    },
    {
      "epoch": 4.093567251461988,
      "grad_norm": 18.316112518310547,
      "learning_rate": 1.486868686868687e-05,
      "loss": 0.4395,
      "step": 528
    },
    {
      "epoch": 4.109161793372319,
      "grad_norm": 20.00517463684082,
      "learning_rate": 1.484848484848485e-05,
      "loss": 0.3822,
      "step": 530
    },
    {
      "epoch": 4.124756335282651,
      "grad_norm": 20.621774673461914,
      "learning_rate": 1.482828282828283e-05,
      "loss": 0.4545,
      "step": 532
    },
    {
      "epoch": 4.140350877192983,
      "grad_norm": 22.04227066040039,
      "learning_rate": 1.4808080808080809e-05,
      "loss": 0.4618,
      "step": 534
    },
    {
      "epoch": 4.155945419103314,
      "grad_norm": 19.592275619506836,
      "learning_rate": 1.478787878787879e-05,
      "loss": 0.5037,
      "step": 536
    },
    {
      "epoch": 4.171539961013645,
      "grad_norm": 19.03037452697754,
      "learning_rate": 1.4767676767676769e-05,
      "loss": 0.4052,
      "step": 538
    },
    {
      "epoch": 4.187134502923977,
      "grad_norm": 16.995141983032227,
      "learning_rate": 1.4747474747474747e-05,
      "loss": 0.3706,
      "step": 540
    },
    {
      "epoch": 4.202729044834308,
      "grad_norm": 13.530309677124023,
      "learning_rate": 1.4727272727272728e-05,
      "loss": 0.3503,
      "step": 542
    },
    {
      "epoch": 4.218323586744639,
      "grad_norm": 15.4989013671875,
      "learning_rate": 1.4707070707070707e-05,
      "loss": 0.357,
      "step": 544
    },
    {
      "epoch": 4.23391812865497,
      "grad_norm": 13.360760688781738,
      "learning_rate": 1.4686868686868687e-05,
      "loss": 0.3566,
      "step": 546
    },
    {
      "epoch": 4.249512670565302,
      "grad_norm": 16.127479553222656,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 0.3666,
      "step": 548
    },
    {
      "epoch": 4.265107212475634,
      "grad_norm": 14.379273414611816,
      "learning_rate": 1.4646464646464649e-05,
      "loss": 0.3137,
      "step": 550
    },
    {
      "epoch": 4.280701754385965,
      "grad_norm": 18.115793228149414,
      "learning_rate": 1.4626262626262629e-05,
      "loss": 0.3356,
      "step": 552
    },
    {
      "epoch": 4.296296296296296,
      "grad_norm": 16.825820922851562,
      "learning_rate": 1.4606060606060608e-05,
      "loss": 0.3934,
      "step": 554
    },
    {
      "epoch": 4.311890838206628,
      "grad_norm": 17.904672622680664,
      "learning_rate": 1.4585858585858587e-05,
      "loss": 0.4061,
      "step": 556
    },
    {
      "epoch": 4.3274853801169595,
      "grad_norm": 22.389963150024414,
      "learning_rate": 1.4565656565656567e-05,
      "loss": 0.4462,
      "step": 558
    },
    {
      "epoch": 4.34307992202729,
      "grad_norm": 25.40640640258789,
      "learning_rate": 1.4545454545454546e-05,
      "loss": 0.4533,
      "step": 560
    },
    {
      "epoch": 4.358674463937621,
      "grad_norm": 19.17026710510254,
      "learning_rate": 1.4525252525252527e-05,
      "loss": 0.4157,
      "step": 562
    },
    {
      "epoch": 4.374269005847953,
      "grad_norm": 16.759145736694336,
      "learning_rate": 1.4505050505050506e-05,
      "loss": 0.3349,
      "step": 564
    },
    {
      "epoch": 4.389863547758285,
      "grad_norm": 19.695829391479492,
      "learning_rate": 1.4484848484848486e-05,
      "loss": 0.4605,
      "step": 566
    },
    {
      "epoch": 4.405458089668616,
      "grad_norm": 24.582284927368164,
      "learning_rate": 1.4464646464646465e-05,
      "loss": 0.4285,
      "step": 568
    },
    {
      "epoch": 4.421052631578947,
      "grad_norm": 21.083343505859375,
      "learning_rate": 1.4444444444444446e-05,
      "loss": 0.3742,
      "step": 570
    },
    {
      "epoch": 4.436647173489279,
      "grad_norm": 19.101409912109375,
      "learning_rate": 1.4424242424242425e-05,
      "loss": 0.3103,
      "step": 572
    },
    {
      "epoch": 4.4522417153996106,
      "grad_norm": 22.79187774658203,
      "learning_rate": 1.4404040404040405e-05,
      "loss": 0.4565,
      "step": 574
    },
    {
      "epoch": 4.4678362573099415,
      "grad_norm": 20.518339157104492,
      "learning_rate": 1.4383838383838384e-05,
      "loss": 0.5144,
      "step": 576
    },
    {
      "epoch": 4.483430799220272,
      "grad_norm": 15.13153076171875,
      "learning_rate": 1.4363636363636365e-05,
      "loss": 0.4568,
      "step": 578
    },
    {
      "epoch": 4.499025341130604,
      "grad_norm": 19.265207290649414,
      "learning_rate": 1.4343434343434344e-05,
      "loss": 0.5342,
      "step": 580
    },
    {
      "epoch": 4.514619883040936,
      "grad_norm": 18.325647354125977,
      "learning_rate": 1.4323232323232324e-05,
      "loss": 0.2749,
      "step": 582
    },
    {
      "epoch": 4.530214424951267,
      "grad_norm": 16.40545082092285,
      "learning_rate": 1.4303030303030305e-05,
      "loss": 0.297,
      "step": 584
    },
    {
      "epoch": 4.545808966861598,
      "grad_norm": 17.6346435546875,
      "learning_rate": 1.4282828282828285e-05,
      "loss": 0.3162,
      "step": 586
    },
    {
      "epoch": 4.56140350877193,
      "grad_norm": 20.756378173828125,
      "learning_rate": 1.4262626262626264e-05,
      "loss": 0.4594,
      "step": 588
    },
    {
      "epoch": 4.576998050682262,
      "grad_norm": 18.912256240844727,
      "learning_rate": 1.4242424242424245e-05,
      "loss": 0.5498,
      "step": 590
    },
    {
      "epoch": 4.592592592592593,
      "grad_norm": 20.340238571166992,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.4717,
      "step": 592
    },
    {
      "epoch": 4.6081871345029235,
      "grad_norm": 17.36246681213379,
      "learning_rate": 1.4202020202020204e-05,
      "loss": 0.4988,
      "step": 594
    },
    {
      "epoch": 4.623781676413255,
      "grad_norm": 21.10388946533203,
      "learning_rate": 1.4181818181818183e-05,
      "loss": 0.5024,
      "step": 596
    },
    {
      "epoch": 4.639376218323587,
      "grad_norm": 22.87828826904297,
      "learning_rate": 1.4161616161616164e-05,
      "loss": 0.4343,
      "step": 598
    },
    {
      "epoch": 4.654970760233918,
      "grad_norm": 16.714113235473633,
      "learning_rate": 1.4141414141414143e-05,
      "loss": 0.392,
      "step": 600
    },
    {
      "epoch": 4.670565302144249,
      "grad_norm": 24.63931655883789,
      "learning_rate": 1.4121212121212121e-05,
      "loss": 0.3474,
      "step": 602
    },
    {
      "epoch": 4.686159844054581,
      "grad_norm": 17.79459571838379,
      "learning_rate": 1.4101010101010102e-05,
      "loss": 0.3856,
      "step": 604
    },
    {
      "epoch": 4.701754385964913,
      "grad_norm": 19.827985763549805,
      "learning_rate": 1.4080808080808081e-05,
      "loss": 0.3218,
      "step": 606
    },
    {
      "epoch": 4.717348927875244,
      "grad_norm": 24.131160736083984,
      "learning_rate": 1.4060606060606061e-05,
      "loss": 0.3242,
      "step": 608
    },
    {
      "epoch": 4.732943469785575,
      "grad_norm": 21.11309242248535,
      "learning_rate": 1.404040404040404e-05,
      "loss": 0.4277,
      "step": 610
    },
    {
      "epoch": 4.748538011695906,
      "grad_norm": 15.01441478729248,
      "learning_rate": 1.4020202020202021e-05,
      "loss": 0.5127,
      "step": 612
    },
    {
      "epoch": 4.764132553606238,
      "grad_norm": 32.87367248535156,
      "learning_rate": 1.4e-05,
      "loss": 0.3426,
      "step": 614
    },
    {
      "epoch": 4.779727095516569,
      "grad_norm": 15.503693580627441,
      "learning_rate": 1.397979797979798e-05,
      "loss": 0.3188,
      "step": 616
    },
    {
      "epoch": 4.7953216374269,
      "grad_norm": 20.76970863342285,
      "learning_rate": 1.3959595959595963e-05,
      "loss": 0.3528,
      "step": 618
    },
    {
      "epoch": 4.810916179337232,
      "grad_norm": 17.758752822875977,
      "learning_rate": 1.3939393939393942e-05,
      "loss": 0.411,
      "step": 620
    },
    {
      "epoch": 4.826510721247564,
      "grad_norm": 22.710962295532227,
      "learning_rate": 1.391919191919192e-05,
      "loss": 0.4167,
      "step": 622
    },
    {
      "epoch": 4.842105263157895,
      "grad_norm": 14.634671211242676,
      "learning_rate": 1.3898989898989901e-05,
      "loss": 0.3356,
      "step": 624
    },
    {
      "epoch": 4.857699805068226,
      "grad_norm": 16.925399780273438,
      "learning_rate": 1.387878787878788e-05,
      "loss": 0.3261,
      "step": 626
    },
    {
      "epoch": 4.8732943469785575,
      "grad_norm": 22.785926818847656,
      "learning_rate": 1.385858585858586e-05,
      "loss": 0.4223,
      "step": 628
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 20.508907318115234,
      "learning_rate": 1.383838383838384e-05,
      "loss": 0.4166,
      "step": 630
    },
    {
      "epoch": 4.90448343079922,
      "grad_norm": 21.039230346679688,
      "learning_rate": 1.381818181818182e-05,
      "loss": 0.4862,
      "step": 632
    },
    {
      "epoch": 4.920077972709551,
      "grad_norm": 53.930213928222656,
      "learning_rate": 1.3797979797979799e-05,
      "loss": 0.3811,
      "step": 634
    },
    {
      "epoch": 4.935672514619883,
      "grad_norm": 17.039722442626953,
      "learning_rate": 1.377777777777778e-05,
      "loss": 0.3757,
      "step": 636
    },
    {
      "epoch": 4.951267056530215,
      "grad_norm": 20.530696868896484,
      "learning_rate": 1.3757575757575758e-05,
      "loss": 0.412,
      "step": 638
    },
    {
      "epoch": 4.966861598440546,
      "grad_norm": 17.25634765625,
      "learning_rate": 1.3737373737373739e-05,
      "loss": 0.3175,
      "step": 640
    },
    {
      "epoch": 4.982456140350877,
      "grad_norm": 16.321882247924805,
      "learning_rate": 1.3717171717171718e-05,
      "loss": 0.3622,
      "step": 642
    },
    {
      "epoch": 4.9980506822612085,
      "grad_norm": 20.757490158081055,
      "learning_rate": 1.3696969696969698e-05,
      "loss": 0.4408,
      "step": 644
    },
    {
      "epoch": 5.007797270955166,
      "grad_norm": 12.209329605102539,
      "learning_rate": 1.3676767676767677e-05,
      "loss": 0.0952,
      "step": 646
    },
    {
      "epoch": 5.023391812865497,
      "grad_norm": 14.27968692779541,
      "learning_rate": 1.3656565656565656e-05,
      "loss": 0.3185,
      "step": 648
    },
    {
      "epoch": 5.038986354775829,
      "grad_norm": 15.57931137084961,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 0.3506,
      "step": 650
    },
    {
      "epoch": 5.05458089668616,
      "grad_norm": 16.690004348754883,
      "learning_rate": 1.3616161616161619e-05,
      "loss": 0.2297,
      "step": 652
    },
    {
      "epoch": 5.0701754385964914,
      "grad_norm": 20.116975784301758,
      "learning_rate": 1.3595959595959598e-05,
      "loss": 0.2772,
      "step": 654
    },
    {
      "epoch": 5.085769980506822,
      "grad_norm": 26.81493377685547,
      "learning_rate": 1.3575757575757578e-05,
      "loss": 0.3503,
      "step": 656
    },
    {
      "epoch": 5.101364522417154,
      "grad_norm": 45.5789794921875,
      "learning_rate": 1.3555555555555557e-05,
      "loss": 0.3396,
      "step": 658
    },
    {
      "epoch": 5.116959064327485,
      "grad_norm": 22.319732666015625,
      "learning_rate": 1.3535353535353538e-05,
      "loss": 0.2548,
      "step": 660
    },
    {
      "epoch": 5.132553606237817,
      "grad_norm": 26.534053802490234,
      "learning_rate": 1.3515151515151517e-05,
      "loss": 0.346,
      "step": 662
    },
    {
      "epoch": 5.148148148148148,
      "grad_norm": 38.71217727661133,
      "learning_rate": 1.3494949494949497e-05,
      "loss": 0.3144,
      "step": 664
    },
    {
      "epoch": 5.16374269005848,
      "grad_norm": 16.162227630615234,
      "learning_rate": 1.3474747474747476e-05,
      "loss": 0.3412,
      "step": 666
    },
    {
      "epoch": 5.179337231968811,
      "grad_norm": 21.377426147460938,
      "learning_rate": 1.3454545454545455e-05,
      "loss": 0.5162,
      "step": 668
    },
    {
      "epoch": 5.1949317738791425,
      "grad_norm": 19.26899528503418,
      "learning_rate": 1.3434343434343436e-05,
      "loss": 0.3252,
      "step": 670
    },
    {
      "epoch": 5.2105263157894735,
      "grad_norm": 104.66879272460938,
      "learning_rate": 1.3414141414141414e-05,
      "loss": 0.2943,
      "step": 672
    },
    {
      "epoch": 5.226120857699805,
      "grad_norm": 23.08385467529297,
      "learning_rate": 1.3393939393939395e-05,
      "loss": 0.4359,
      "step": 674
    },
    {
      "epoch": 5.241715399610136,
      "grad_norm": 22.011972427368164,
      "learning_rate": 1.3373737373737374e-05,
      "loss": 0.3636,
      "step": 676
    },
    {
      "epoch": 5.257309941520468,
      "grad_norm": 17.47159767150879,
      "learning_rate": 1.3353535353535354e-05,
      "loss": 0.3497,
      "step": 678
    },
    {
      "epoch": 5.272904483430799,
      "grad_norm": 23.082773208618164,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.2244,
      "step": 680
    },
    {
      "epoch": 5.288499025341131,
      "grad_norm": 24.580232620239258,
      "learning_rate": 1.3313131313131314e-05,
      "loss": 0.2004,
      "step": 682
    },
    {
      "epoch": 5.304093567251462,
      "grad_norm": 28.536569595336914,
      "learning_rate": 1.3292929292929293e-05,
      "loss": 0.3347,
      "step": 684
    },
    {
      "epoch": 5.319688109161794,
      "grad_norm": 22.219825744628906,
      "learning_rate": 1.3272727272727275e-05,
      "loss": 0.434,
      "step": 686
    },
    {
      "epoch": 5.3352826510721245,
      "grad_norm": 28.938922882080078,
      "learning_rate": 1.3252525252525254e-05,
      "loss": 0.2082,
      "step": 688
    },
    {
      "epoch": 5.350877192982456,
      "grad_norm": 19.071374893188477,
      "learning_rate": 1.3232323232323234e-05,
      "loss": 0.3964,
      "step": 690
    },
    {
      "epoch": 5.366471734892787,
      "grad_norm": 20.94932746887207,
      "learning_rate": 1.3212121212121213e-05,
      "loss": 0.2731,
      "step": 692
    },
    {
      "epoch": 5.382066276803119,
      "grad_norm": 26.8670597076416,
      "learning_rate": 1.3191919191919194e-05,
      "loss": 0.1995,
      "step": 694
    },
    {
      "epoch": 5.39766081871345,
      "grad_norm": 14.625631332397461,
      "learning_rate": 1.3171717171717173e-05,
      "loss": 0.4418,
      "step": 696
    },
    {
      "epoch": 5.413255360623782,
      "grad_norm": 37.07080078125,
      "learning_rate": 1.3151515151515153e-05,
      "loss": 0.3032,
      "step": 698
    },
    {
      "epoch": 5.428849902534113,
      "grad_norm": 19.26723861694336,
      "learning_rate": 1.3131313131313132e-05,
      "loss": 0.2654,
      "step": 700
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 21.405914306640625,
      "learning_rate": 1.3111111111111113e-05,
      "loss": 0.2677,
      "step": 702
    },
    {
      "epoch": 5.460038986354776,
      "grad_norm": 24.951017379760742,
      "learning_rate": 1.3090909090909092e-05,
      "loss": 0.3538,
      "step": 704
    },
    {
      "epoch": 5.475633528265107,
      "grad_norm": 17.997365951538086,
      "learning_rate": 1.3070707070707072e-05,
      "loss": 0.4045,
      "step": 706
    },
    {
      "epoch": 5.491228070175438,
      "grad_norm": 24.894744873046875,
      "learning_rate": 1.3050505050505051e-05,
      "loss": 0.448,
      "step": 708
    },
    {
      "epoch": 5.50682261208577,
      "grad_norm": 17.66102409362793,
      "learning_rate": 1.3030303030303032e-05,
      "loss": 0.3203,
      "step": 710
    },
    {
      "epoch": 5.522417153996101,
      "grad_norm": 47.99385070800781,
      "learning_rate": 1.301010101010101e-05,
      "loss": 0.3084,
      "step": 712
    },
    {
      "epoch": 5.538011695906433,
      "grad_norm": 22.22443962097168,
      "learning_rate": 1.298989898989899e-05,
      "loss": 0.3328,
      "step": 714
    },
    {
      "epoch": 5.553606237816764,
      "grad_norm": 35.963436126708984,
      "learning_rate": 1.296969696969697e-05,
      "loss": 0.4991,
      "step": 716
    },
    {
      "epoch": 5.569200779727096,
      "grad_norm": 24.538593292236328,
      "learning_rate": 1.2949494949494949e-05,
      "loss": 0.4201,
      "step": 718
    },
    {
      "epoch": 5.584795321637427,
      "grad_norm": 19.91373062133789,
      "learning_rate": 1.2929292929292931e-05,
      "loss": 0.5394,
      "step": 720
    },
    {
      "epoch": 5.6003898635477585,
      "grad_norm": 18.174100875854492,
      "learning_rate": 1.2909090909090912e-05,
      "loss": 0.2809,
      "step": 722
    },
    {
      "epoch": 5.615984405458089,
      "grad_norm": 31.927268981933594,
      "learning_rate": 1.288888888888889e-05,
      "loss": 0.3179,
      "step": 724
    },
    {
      "epoch": 5.631578947368421,
      "grad_norm": 21.522754669189453,
      "learning_rate": 1.2868686868686871e-05,
      "loss": 0.3322,
      "step": 726
    },
    {
      "epoch": 5.647173489278752,
      "grad_norm": 25.305055618286133,
      "learning_rate": 1.284848484848485e-05,
      "loss": 0.366,
      "step": 728
    },
    {
      "epoch": 5.662768031189084,
      "grad_norm": 23.67844581604004,
      "learning_rate": 1.2828282828282829e-05,
      "loss": 0.3672,
      "step": 730
    },
    {
      "epoch": 5.678362573099415,
      "grad_norm": 22.781063079833984,
      "learning_rate": 1.280808080808081e-05,
      "loss": 0.2689,
      "step": 732
    },
    {
      "epoch": 5.693957115009747,
      "grad_norm": 38.360382080078125,
      "learning_rate": 1.2787878787878788e-05,
      "loss": 0.1943,
      "step": 734
    },
    {
      "epoch": 5.709551656920078,
      "grad_norm": 19.507251739501953,
      "learning_rate": 1.2767676767676769e-05,
      "loss": 0.2695,
      "step": 736
    },
    {
      "epoch": 5.7251461988304095,
      "grad_norm": 23.681026458740234,
      "learning_rate": 1.2747474747474748e-05,
      "loss": 0.3486,
      "step": 738
    },
    {
      "epoch": 5.7407407407407405,
      "grad_norm": 15.482949256896973,
      "learning_rate": 1.2727272727272728e-05,
      "loss": 0.3197,
      "step": 740
    },
    {
      "epoch": 5.756335282651072,
      "grad_norm": 23.304555892944336,
      "learning_rate": 1.2707070707070707e-05,
      "loss": 0.4526,
      "step": 742
    },
    {
      "epoch": 5.771929824561403,
      "grad_norm": 27.280254364013672,
      "learning_rate": 1.2686868686868688e-05,
      "loss": 0.3635,
      "step": 744
    },
    {
      "epoch": 5.787524366471735,
      "grad_norm": 21.44009017944336,
      "learning_rate": 1.2666666666666667e-05,
      "loss": 0.3083,
      "step": 746
    },
    {
      "epoch": 5.803118908382066,
      "grad_norm": 24.783769607543945,
      "learning_rate": 1.2646464646464647e-05,
      "loss": 0.3246,
      "step": 748
    },
    {
      "epoch": 5.818713450292398,
      "grad_norm": 19.93307113647461,
      "learning_rate": 1.2626262626262626e-05,
      "loss": 0.2815,
      "step": 750
    },
    {
      "epoch": 5.834307992202729,
      "grad_norm": 24.280916213989258,
      "learning_rate": 1.2606060606060607e-05,
      "loss": 0.3851,
      "step": 752
    },
    {
      "epoch": 5.849902534113061,
      "grad_norm": 22.28690528869629,
      "learning_rate": 1.2585858585858587e-05,
      "loss": 0.1959,
      "step": 754
    },
    {
      "epoch": 5.8654970760233915,
      "grad_norm": 21.141584396362305,
      "learning_rate": 1.2565656565656568e-05,
      "loss": 0.2063,
      "step": 756
    },
    {
      "epoch": 5.881091617933723,
      "grad_norm": 16.761564254760742,
      "learning_rate": 1.2545454545454547e-05,
      "loss": 0.3988,
      "step": 758
    },
    {
      "epoch": 5.896686159844054,
      "grad_norm": 24.857942581176758,
      "learning_rate": 1.2525252525252527e-05,
      "loss": 0.3596,
      "step": 760
    },
    {
      "epoch": 5.912280701754386,
      "grad_norm": 23.948213577270508,
      "learning_rate": 1.2505050505050506e-05,
      "loss": 0.2895,
      "step": 762
    },
    {
      "epoch": 5.927875243664717,
      "grad_norm": 27.772560119628906,
      "learning_rate": 1.2484848484848487e-05,
      "loss": 0.3793,
      "step": 764
    },
    {
      "epoch": 5.943469785575049,
      "grad_norm": 22.26348304748535,
      "learning_rate": 1.2464646464646466e-05,
      "loss": 0.2664,
      "step": 766
    },
    {
      "epoch": 5.95906432748538,
      "grad_norm": 17.9086971282959,
      "learning_rate": 1.2444444444444446e-05,
      "loss": 0.3804,
      "step": 768
    },
    {
      "epoch": 5.974658869395712,
      "grad_norm": 21.846763610839844,
      "learning_rate": 1.2424242424242425e-05,
      "loss": 0.276,
      "step": 770
    },
    {
      "epoch": 5.990253411306043,
      "grad_norm": 23.326644897460938,
      "learning_rate": 1.2404040404040406e-05,
      "loss": 0.3018,
      "step": 772
    },
    {
      "epoch": 6.0,
      "grad_norm": 13.522562026977539,
      "learning_rate": 1.2383838383838385e-05,
      "loss": 0.2689,
      "step": 774
    },
    {
      "epoch": 6.015594541910332,
      "grad_norm": 23.752399444580078,
      "learning_rate": 1.2363636363636364e-05,
      "loss": 0.2706,
      "step": 776
    },
    {
      "epoch": 6.031189083820663,
      "grad_norm": 15.952486038208008,
      "learning_rate": 1.2343434343434344e-05,
      "loss": 0.3821,
      "step": 778
    },
    {
      "epoch": 6.046783625730994,
      "grad_norm": 20.936573028564453,
      "learning_rate": 1.2323232323232323e-05,
      "loss": 0.3391,
      "step": 780
    },
    {
      "epoch": 6.0623781676413255,
      "grad_norm": 22.19004249572754,
      "learning_rate": 1.2303030303030304e-05,
      "loss": 0.2389,
      "step": 782
    },
    {
      "epoch": 6.077972709551657,
      "grad_norm": 21.602813720703125,
      "learning_rate": 1.2282828282828282e-05,
      "loss": 0.306,
      "step": 784
    },
    {
      "epoch": 6.093567251461988,
      "grad_norm": 23.044679641723633,
      "learning_rate": 1.2262626262626263e-05,
      "loss": 0.3111,
      "step": 786
    },
    {
      "epoch": 6.109161793372319,
      "grad_norm": 23.44725227355957,
      "learning_rate": 1.2242424242424242e-05,
      "loss": 0.3153,
      "step": 788
    },
    {
      "epoch": 6.124756335282651,
      "grad_norm": 25.531375885009766,
      "learning_rate": 1.2222222222222224e-05,
      "loss": 0.2547,
      "step": 790
    },
    {
      "epoch": 6.140350877192983,
      "grad_norm": 33.6808967590332,
      "learning_rate": 1.2202020202020203e-05,
      "loss": 0.1426,
      "step": 792
    },
    {
      "epoch": 6.155945419103314,
      "grad_norm": 18.5185489654541,
      "learning_rate": 1.2181818181818184e-05,
      "loss": 0.2175,
      "step": 794
    },
    {
      "epoch": 6.171539961013645,
      "grad_norm": 28.560726165771484,
      "learning_rate": 1.2161616161616162e-05,
      "loss": 0.3425,
      "step": 796
    },
    {
      "epoch": 6.187134502923977,
      "grad_norm": 23.557992935180664,
      "learning_rate": 1.2141414141414143e-05,
      "loss": 0.3455,
      "step": 798
    },
    {
      "epoch": 6.202729044834308,
      "grad_norm": 22.779245376586914,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 0.2879,
      "step": 800
    },
    {
      "epoch": 6.218323586744639,
      "grad_norm": 26.96988868713379,
      "learning_rate": 1.2101010101010103e-05,
      "loss": 0.318,
      "step": 802
    },
    {
      "epoch": 6.23391812865497,
      "grad_norm": 19.517868041992188,
      "learning_rate": 1.2080808080808081e-05,
      "loss": 0.1731,
      "step": 804
    },
    {
      "epoch": 6.249512670565302,
      "grad_norm": 20.507694244384766,
      "learning_rate": 1.2060606060606062e-05,
      "loss": 0.3043,
      "step": 806
    },
    {
      "epoch": 6.265107212475634,
      "grad_norm": 19.593154907226562,
      "learning_rate": 1.2040404040404041e-05,
      "loss": 0.3044,
      "step": 808
    },
    {
      "epoch": 6.280701754385965,
      "grad_norm": 19.88922882080078,
      "learning_rate": 1.2020202020202021e-05,
      "loss": 0.3011,
      "step": 810
    },
    {
      "epoch": 6.296296296296296,
      "grad_norm": 31.234453201293945,
      "learning_rate": 1.2e-05,
      "loss": 0.1817,
      "step": 812
    },
    {
      "epoch": 6.311890838206628,
      "grad_norm": 26.046567916870117,
      "learning_rate": 1.1979797979797981e-05,
      "loss": 0.2259,
      "step": 814
    },
    {
      "epoch": 6.3274853801169595,
      "grad_norm": 23.326519012451172,
      "learning_rate": 1.195959595959596e-05,
      "loss": 0.2446,
      "step": 816
    },
    {
      "epoch": 6.34307992202729,
      "grad_norm": 27.364364624023438,
      "learning_rate": 1.193939393939394e-05,
      "loss": 0.2957,
      "step": 818
    },
    {
      "epoch": 6.358674463937621,
      "grad_norm": 23.458967208862305,
      "learning_rate": 1.191919191919192e-05,
      "loss": 0.2262,
      "step": 820
    },
    {
      "epoch": 6.374269005847953,
      "grad_norm": 27.125064849853516,
      "learning_rate": 1.1898989898989898e-05,
      "loss": 0.275,
      "step": 822
    },
    {
      "epoch": 6.389863547758285,
      "grad_norm": 24.33766746520996,
      "learning_rate": 1.187878787878788e-05,
      "loss": 0.3394,
      "step": 824
    },
    {
      "epoch": 6.405458089668616,
      "grad_norm": 18.650413513183594,
      "learning_rate": 1.1858585858585861e-05,
      "loss": 0.2146,
      "step": 826
    },
    {
      "epoch": 6.421052631578947,
      "grad_norm": 30.674863815307617,
      "learning_rate": 1.183838383838384e-05,
      "loss": 0.3065,
      "step": 828
    },
    {
      "epoch": 6.436647173489279,
      "grad_norm": 17.62580108642578,
      "learning_rate": 1.181818181818182e-05,
      "loss": 0.2992,
      "step": 830
    },
    {
      "epoch": 6.4522417153996106,
      "grad_norm": 34.775997161865234,
      "learning_rate": 1.17979797979798e-05,
      "loss": 0.3864,
      "step": 832
    },
    {
      "epoch": 6.4678362573099415,
      "grad_norm": 19.851154327392578,
      "learning_rate": 1.177777777777778e-05,
      "loss": 0.2842,
      "step": 834
    },
    {
      "epoch": 6.483430799220272,
      "grad_norm": 20.12225341796875,
      "learning_rate": 1.1757575757575759e-05,
      "loss": 0.3248,
      "step": 836
    },
    {
      "epoch": 6.499025341130604,
      "grad_norm": 23.497241973876953,
      "learning_rate": 1.1737373737373738e-05,
      "loss": 0.2273,
      "step": 838
    },
    {
      "epoch": 6.514619883040936,
      "grad_norm": 19.766504287719727,
      "learning_rate": 1.1717171717171718e-05,
      "loss": 0.1515,
      "step": 840
    },
    {
      "epoch": 6.530214424951267,
      "grad_norm": 25.174476623535156,
      "learning_rate": 1.1696969696969697e-05,
      "loss": 0.2458,
      "step": 842
    },
    {
      "epoch": 6.545808966861598,
      "grad_norm": 23.80866050720215,
      "learning_rate": 1.1676767676767678e-05,
      "loss": 0.183,
      "step": 844
    },
    {
      "epoch": 6.56140350877193,
      "grad_norm": 19.749975204467773,
      "learning_rate": 1.1656565656565656e-05,
      "loss": 0.2307,
      "step": 846
    },
    {
      "epoch": 6.576998050682262,
      "grad_norm": 28.492137908935547,
      "learning_rate": 1.1636363636363637e-05,
      "loss": 0.3347,
      "step": 848
    },
    {
      "epoch": 6.592592592592593,
      "grad_norm": 21.719621658325195,
      "learning_rate": 1.1616161616161616e-05,
      "loss": 0.3332,
      "step": 850
    },
    {
      "epoch": 6.6081871345029235,
      "grad_norm": 25.104549407958984,
      "learning_rate": 1.1595959595959597e-05,
      "loss": 0.2575,
      "step": 852
    },
    {
      "epoch": 6.623781676413255,
      "grad_norm": 19.817087173461914,
      "learning_rate": 1.1575757575757575e-05,
      "loss": 0.151,
      "step": 854
    },
    {
      "epoch": 6.639376218323587,
      "grad_norm": 25.436477661132812,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 0.3122,
      "step": 856
    },
    {
      "epoch": 6.654970760233918,
      "grad_norm": 28.289047241210938,
      "learning_rate": 1.1535353535353537e-05,
      "loss": 0.1952,
      "step": 858
    },
    {
      "epoch": 6.670565302144249,
      "grad_norm": 29.1168270111084,
      "learning_rate": 1.1515151515151517e-05,
      "loss": 0.249,
      "step": 860
    },
    {
      "epoch": 6.686159844054581,
      "grad_norm": 22.051624298095703,
      "learning_rate": 1.1494949494949496e-05,
      "loss": 0.2943,
      "step": 862
    },
    {
      "epoch": 6.701754385964913,
      "grad_norm": 28.60834312438965,
      "learning_rate": 1.1474747474747477e-05,
      "loss": 0.1749,
      "step": 864
    },
    {
      "epoch": 6.717348927875244,
      "grad_norm": 27.04265785217285,
      "learning_rate": 1.1454545454545455e-05,
      "loss": 0.3044,
      "step": 866
    },
    {
      "epoch": 6.732943469785575,
      "grad_norm": 16.803682327270508,
      "learning_rate": 1.1434343434343436e-05,
      "loss": 0.2071,
      "step": 868
    },
    {
      "epoch": 6.748538011695906,
      "grad_norm": 28.948841094970703,
      "learning_rate": 1.1414141414141415e-05,
      "loss": 0.2768,
      "step": 870
    },
    {
      "epoch": 6.764132553606238,
      "grad_norm": 21.147260665893555,
      "learning_rate": 1.1393939393939395e-05,
      "loss": 0.3307,
      "step": 872
    },
    {
      "epoch": 6.779727095516569,
      "grad_norm": 22.62190055847168,
      "learning_rate": 1.1373737373737374e-05,
      "loss": 0.1868,
      "step": 874
    },
    {
      "epoch": 6.7953216374269,
      "grad_norm": 23.63344955444336,
      "learning_rate": 1.1353535353535355e-05,
      "loss": 0.2496,
      "step": 876
    },
    {
      "epoch": 6.810916179337232,
      "grad_norm": 26.268381118774414,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.2572,
      "step": 878
    },
    {
      "epoch": 6.826510721247564,
      "grad_norm": 30.421682357788086,
      "learning_rate": 1.1313131313131314e-05,
      "loss": 0.2786,
      "step": 880
    },
    {
      "epoch": 6.842105263157895,
      "grad_norm": 25.843643188476562,
      "learning_rate": 1.1292929292929293e-05,
      "loss": 0.2053,
      "step": 882
    },
    {
      "epoch": 6.857699805068226,
      "grad_norm": 26.73390007019043,
      "learning_rate": 1.1272727272727272e-05,
      "loss": 0.2082,
      "step": 884
    },
    {
      "epoch": 6.8732943469785575,
      "grad_norm": 38.480255126953125,
      "learning_rate": 1.1252525252525253e-05,
      "loss": 0.2429,
      "step": 886
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 32.16217041015625,
      "learning_rate": 1.1232323232323232e-05,
      "loss": 0.259,
      "step": 888
    },
    {
      "epoch": 6.90448343079922,
      "grad_norm": 28.25645637512207,
      "learning_rate": 1.1212121212121212e-05,
      "loss": 0.264,
      "step": 890
    },
    {
      "epoch": 6.920077972709551,
      "grad_norm": 24.14026641845703,
      "learning_rate": 1.1191919191919194e-05,
      "loss": 0.3258,
      "step": 892
    },
    {
      "epoch": 6.935672514619883,
      "grad_norm": 15.677886009216309,
      "learning_rate": 1.1171717171717173e-05,
      "loss": 0.2711,
      "step": 894
    },
    {
      "epoch": 6.951267056530215,
      "grad_norm": 28.512863159179688,
      "learning_rate": 1.1151515151515154e-05,
      "loss": 0.2718,
      "step": 896
    },
    {
      "epoch": 6.966861598440546,
      "grad_norm": 26.819448471069336,
      "learning_rate": 1.1131313131313133e-05,
      "loss": 0.4169,
      "step": 898
    },
    {
      "epoch": 6.982456140350877,
      "grad_norm": 28.763294219970703,
      "learning_rate": 1.1111111111111113e-05,
      "loss": 0.2923,
      "step": 900
    },
    {
      "epoch": 6.9980506822612085,
      "grad_norm": 28.063745498657227,
      "learning_rate": 1.1090909090909092e-05,
      "loss": 0.2397,
      "step": 902
    },
    {
      "epoch": 7.007797270955166,
      "grad_norm": 13.515007972717285,
      "learning_rate": 1.1070707070707071e-05,
      "loss": 0.1388,
      "step": 904
    },
    {
      "epoch": 7.023391812865497,
      "grad_norm": 16.358901977539062,
      "learning_rate": 1.1050505050505052e-05,
      "loss": 0.2778,
      "step": 906
    },
    {
      "epoch": 7.038986354775829,
      "grad_norm": 17.593416213989258,
      "learning_rate": 1.103030303030303e-05,
      "loss": 0.2275,
      "step": 908
    },
    {
      "epoch": 7.05458089668616,
      "grad_norm": 18.42652702331543,
      "learning_rate": 1.1010101010101011e-05,
      "loss": 0.1308,
      "step": 910
    },
    {
      "epoch": 7.0701754385964914,
      "grad_norm": 10.553998947143555,
      "learning_rate": 1.098989898989899e-05,
      "loss": 0.173,
      "step": 912
    },
    {
      "epoch": 7.085769980506822,
      "grad_norm": 40.146949768066406,
      "learning_rate": 1.096969696969697e-05,
      "loss": 0.3135,
      "step": 914
    },
    {
      "epoch": 7.101364522417154,
      "grad_norm": 27.21980857849121,
      "learning_rate": 1.094949494949495e-05,
      "loss": 0.1737,
      "step": 916
    },
    {
      "epoch": 7.116959064327485,
      "grad_norm": 20.563167572021484,
      "learning_rate": 1.092929292929293e-05,
      "loss": 0.1594,
      "step": 918
    },
    {
      "epoch": 7.132553606237817,
      "grad_norm": 18.120555877685547,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.2239,
      "step": 920
    },
    {
      "epoch": 7.148148148148148,
      "grad_norm": 15.541219711303711,
      "learning_rate": 1.088888888888889e-05,
      "loss": 0.193,
      "step": 922
    },
    {
      "epoch": 7.16374269005848,
      "grad_norm": 19.379907608032227,
      "learning_rate": 1.0868686868686868e-05,
      "loss": 0.177,
      "step": 924
    },
    {
      "epoch": 7.179337231968811,
      "grad_norm": 26.74601173400879,
      "learning_rate": 1.084848484848485e-05,
      "loss": 0.2269,
      "step": 926
    },
    {
      "epoch": 7.1949317738791425,
      "grad_norm": 22.261627197265625,
      "learning_rate": 1.082828282828283e-05,
      "loss": 0.1658,
      "step": 928
    },
    {
      "epoch": 7.2105263157894735,
      "grad_norm": 20.375520706176758,
      "learning_rate": 1.080808080808081e-05,
      "loss": 0.1956,
      "step": 930
    },
    {
      "epoch": 7.226120857699805,
      "grad_norm": 17.202476501464844,
      "learning_rate": 1.0787878787878789e-05,
      "loss": 0.1602,
      "step": 932
    },
    {
      "epoch": 7.241715399610136,
      "grad_norm": 18.367355346679688,
      "learning_rate": 1.076767676767677e-05,
      "loss": 0.1268,
      "step": 934
    },
    {
      "epoch": 7.257309941520468,
      "grad_norm": 24.9132080078125,
      "learning_rate": 1.0747474747474748e-05,
      "loss": 0.2147,
      "step": 936
    },
    {
      "epoch": 7.272904483430799,
      "grad_norm": 28.69911766052246,
      "learning_rate": 1.0727272727272729e-05,
      "loss": 0.2096,
      "step": 938
    },
    {
      "epoch": 7.288499025341131,
      "grad_norm": 23.636316299438477,
      "learning_rate": 1.0707070707070708e-05,
      "loss": 0.1478,
      "step": 940
    },
    {
      "epoch": 7.304093567251462,
      "grad_norm": 15.493230819702148,
      "learning_rate": 1.0686868686868688e-05,
      "loss": 0.1882,
      "step": 942
    },
    {
      "epoch": 7.319688109161794,
      "grad_norm": 29.530569076538086,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.2872,
      "step": 944
    },
    {
      "epoch": 7.3352826510721245,
      "grad_norm": 25.722280502319336,
      "learning_rate": 1.0646464646464648e-05,
      "loss": 0.1701,
      "step": 946
    },
    {
      "epoch": 7.350877192982456,
      "grad_norm": 20.51018714904785,
      "learning_rate": 1.0626262626262627e-05,
      "loss": 0.2226,
      "step": 948
    },
    {
      "epoch": 7.366471734892787,
      "grad_norm": 24.362380981445312,
      "learning_rate": 1.0606060606060606e-05,
      "loss": 0.2297,
      "step": 950
    },
    {
      "epoch": 7.382066276803119,
      "grad_norm": 31.770517349243164,
      "learning_rate": 1.0585858585858586e-05,
      "loss": 0.2295,
      "step": 952
    },
    {
      "epoch": 7.39766081871345,
      "grad_norm": 45.7972526550293,
      "learning_rate": 1.0565656565656565e-05,
      "loss": 0.2582,
      "step": 954
    },
    {
      "epoch": 7.413255360623782,
      "grad_norm": 35.04169464111328,
      "learning_rate": 1.0545454545454546e-05,
      "loss": 0.1979,
      "step": 956
    },
    {
      "epoch": 7.428849902534113,
      "grad_norm": 34.25749969482422,
      "learning_rate": 1.0525252525252525e-05,
      "loss": 0.1988,
      "step": 958
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 25.958515167236328,
      "learning_rate": 1.0505050505050507e-05,
      "loss": 0.2854,
      "step": 960
    },
    {
      "epoch": 7.460038986354776,
      "grad_norm": 20.38837432861328,
      "learning_rate": 1.0484848484848487e-05,
      "loss": 0.2714,
      "step": 962
    },
    {
      "epoch": 7.475633528265107,
      "grad_norm": 24.365375518798828,
      "learning_rate": 1.0464646464646466e-05,
      "loss": 0.2315,
      "step": 964
    },
    {
      "epoch": 7.491228070175438,
      "grad_norm": 20.69622039794922,
      "learning_rate": 1.0444444444444445e-05,
      "loss": 0.1411,
      "step": 966
    },
    {
      "epoch": 7.50682261208577,
      "grad_norm": 19.387664794921875,
      "learning_rate": 1.0424242424242426e-05,
      "loss": 0.2035,
      "step": 968
    },
    {
      "epoch": 7.522417153996101,
      "grad_norm": 32.72807693481445,
      "learning_rate": 1.0404040404040405e-05,
      "loss": 0.264,
      "step": 970
    },
    {
      "epoch": 7.538011695906433,
      "grad_norm": 21.400590896606445,
      "learning_rate": 1.0383838383838385e-05,
      "loss": 0.2198,
      "step": 972
    },
    {
      "epoch": 7.553606237816764,
      "grad_norm": 19.947635650634766,
      "learning_rate": 1.0363636363636364e-05,
      "loss": 0.2409,
      "step": 974
    },
    {
      "epoch": 7.569200779727096,
      "grad_norm": 33.72633743286133,
      "learning_rate": 1.0343434343434345e-05,
      "loss": 0.3067,
      "step": 976
    },
    {
      "epoch": 7.584795321637427,
      "grad_norm": 31.669063568115234,
      "learning_rate": 1.0323232323232324e-05,
      "loss": 0.2362,
      "step": 978
    },
    {
      "epoch": 7.6003898635477585,
      "grad_norm": 23.40755844116211,
      "learning_rate": 1.0303030303030304e-05,
      "loss": 0.2779,
      "step": 980
    },
    {
      "epoch": 7.615984405458089,
      "grad_norm": 21.14022445678711,
      "learning_rate": 1.0282828282828283e-05,
      "loss": 0.2183,
      "step": 982
    },
    {
      "epoch": 7.631578947368421,
      "grad_norm": 23.169151306152344,
      "learning_rate": 1.0262626262626264e-05,
      "loss": 0.2251,
      "step": 984
    },
    {
      "epoch": 7.647173489278752,
      "grad_norm": 33.35572814941406,
      "learning_rate": 1.0242424242424242e-05,
      "loss": 0.2735,
      "step": 986
    },
    {
      "epoch": 7.662768031189084,
      "grad_norm": 24.466888427734375,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.151,
      "step": 988
    },
    {
      "epoch": 7.678362573099415,
      "grad_norm": 27.23938751220703,
      "learning_rate": 1.0202020202020202e-05,
      "loss": 0.2144,
      "step": 990
    },
    {
      "epoch": 7.693957115009747,
      "grad_norm": 27.139768600463867,
      "learning_rate": 1.0181818181818182e-05,
      "loss": 0.2643,
      "step": 992
    },
    {
      "epoch": 7.709551656920078,
      "grad_norm": 18.691394805908203,
      "learning_rate": 1.0161616161616163e-05,
      "loss": 0.3379,
      "step": 994
    },
    {
      "epoch": 7.7251461988304095,
      "grad_norm": 23.212387084960938,
      "learning_rate": 1.0141414141414144e-05,
      "loss": 0.2565,
      "step": 996
    },
    {
      "epoch": 7.7407407407407405,
      "grad_norm": 26.48781967163086,
      "learning_rate": 1.0121212121212122e-05,
      "loss": 0.1922,
      "step": 998
    },
    {
      "epoch": 7.756335282651072,
      "grad_norm": 26.905078887939453,
      "learning_rate": 1.0101010101010103e-05,
      "loss": 0.2323,
      "step": 1000
    },
    {
      "epoch": 7.771929824561403,
      "grad_norm": 20.673131942749023,
      "learning_rate": 1.0080808080808082e-05,
      "loss": 0.1426,
      "step": 1002
    },
    {
      "epoch": 7.787524366471735,
      "grad_norm": 28.092548370361328,
      "learning_rate": 1.0060606060606062e-05,
      "loss": 0.1999,
      "step": 1004
    },
    {
      "epoch": 7.803118908382066,
      "grad_norm": 20.9631290435791,
      "learning_rate": 1.0040404040404041e-05,
      "loss": 0.167,
      "step": 1006
    },
    {
      "epoch": 7.818713450292398,
      "grad_norm": 25.58015251159668,
      "learning_rate": 1.0020202020202022e-05,
      "loss": 0.1839,
      "step": 1008
    },
    {
      "epoch": 7.834307992202729,
      "grad_norm": 16.253379821777344,
      "learning_rate": 1e-05,
      "loss": 0.1797,
      "step": 1010
    },
    {
      "epoch": 7.849902534113061,
      "grad_norm": 28.35422134399414,
      "learning_rate": 9.97979797979798e-06,
      "loss": 0.2815,
      "step": 1012
    },
    {
      "epoch": 7.8654970760233915,
      "grad_norm": 21.285497665405273,
      "learning_rate": 9.95959595959596e-06,
      "loss": 0.2462,
      "step": 1014
    },
    {
      "epoch": 7.881091617933723,
      "grad_norm": 18.86545181274414,
      "learning_rate": 9.939393939393939e-06,
      "loss": 0.2335,
      "step": 1016
    },
    {
      "epoch": 7.896686159844054,
      "grad_norm": 28.130815505981445,
      "learning_rate": 9.91919191919192e-06,
      "loss": 0.2691,
      "step": 1018
    },
    {
      "epoch": 7.912280701754386,
      "grad_norm": 20.154481887817383,
      "learning_rate": 9.8989898989899e-06,
      "loss": 0.2658,
      "step": 1020
    },
    {
      "epoch": 7.927875243664717,
      "grad_norm": 25.03174591064453,
      "learning_rate": 9.87878787878788e-06,
      "loss": 0.1933,
      "step": 1022
    },
    {
      "epoch": 7.943469785575049,
      "grad_norm": 32.77711868286133,
      "learning_rate": 9.85858585858586e-06,
      "loss": 0.2058,
      "step": 1024
    },
    {
      "epoch": 7.95906432748538,
      "grad_norm": 30.242033004760742,
      "learning_rate": 9.838383838383839e-06,
      "loss": 0.2224,
      "step": 1026
    },
    {
      "epoch": 7.974658869395712,
      "grad_norm": 22.451751708984375,
      "learning_rate": 9.81818181818182e-06,
      "loss": 0.1917,
      "step": 1028
    },
    {
      "epoch": 7.990253411306043,
      "grad_norm": 22.877714157104492,
      "learning_rate": 9.797979797979798e-06,
      "loss": 0.1834,
      "step": 1030
    },
    {
      "epoch": 8.0,
      "grad_norm": 16.59439468383789,
      "learning_rate": 9.777777777777779e-06,
      "loss": 0.1289,
      "step": 1032
    },
    {
      "epoch": 8.015594541910332,
      "grad_norm": 18.990530014038086,
      "learning_rate": 9.757575757575758e-06,
      "loss": 0.1744,
      "step": 1034
    },
    {
      "epoch": 8.031189083820664,
      "grad_norm": 15.15688705444336,
      "learning_rate": 9.737373737373738e-06,
      "loss": 0.1229,
      "step": 1036
    },
    {
      "epoch": 8.046783625730994,
      "grad_norm": 20.810688018798828,
      "learning_rate": 9.717171717171719e-06,
      "loss": 0.2636,
      "step": 1038
    },
    {
      "epoch": 8.062378167641326,
      "grad_norm": 23.645557403564453,
      "learning_rate": 9.696969696969698e-06,
      "loss": 0.2262,
      "step": 1040
    },
    {
      "epoch": 8.077972709551657,
      "grad_norm": 21.995901107788086,
      "learning_rate": 9.676767676767678e-06,
      "loss": 0.1722,
      "step": 1042
    },
    {
      "epoch": 8.093567251461987,
      "grad_norm": 22.518096923828125,
      "learning_rate": 9.656565656565657e-06,
      "loss": 0.1402,
      "step": 1044
    },
    {
      "epoch": 8.10916179337232,
      "grad_norm": 21.517101287841797,
      "learning_rate": 9.636363636363638e-06,
      "loss": 0.1441,
      "step": 1046
    },
    {
      "epoch": 8.124756335282651,
      "grad_norm": 21.759992599487305,
      "learning_rate": 9.616161616161616e-06,
      "loss": 0.1138,
      "step": 1048
    },
    {
      "epoch": 8.140350877192983,
      "grad_norm": 24.492841720581055,
      "learning_rate": 9.595959595959597e-06,
      "loss": 0.1279,
      "step": 1050
    },
    {
      "epoch": 8.155945419103315,
      "grad_norm": 26.811420440673828,
      "learning_rate": 9.575757575757576e-06,
      "loss": 0.159,
      "step": 1052
    },
    {
      "epoch": 8.171539961013645,
      "grad_norm": 20.007184982299805,
      "learning_rate": 9.555555555555556e-06,
      "loss": 0.2399,
      "step": 1054
    },
    {
      "epoch": 8.187134502923977,
      "grad_norm": 24.521242141723633,
      "learning_rate": 9.535353535353537e-06,
      "loss": 0.11,
      "step": 1056
    },
    {
      "epoch": 8.202729044834308,
      "grad_norm": 28.422147750854492,
      "learning_rate": 9.515151515151516e-06,
      "loss": 0.1738,
      "step": 1058
    },
    {
      "epoch": 8.218323586744638,
      "grad_norm": 24.858488082885742,
      "learning_rate": 9.494949494949497e-06,
      "loss": 0.1667,
      "step": 1060
    },
    {
      "epoch": 8.23391812865497,
      "grad_norm": 22.476852416992188,
      "learning_rate": 9.474747474747475e-06,
      "loss": 0.1163,
      "step": 1062
    },
    {
      "epoch": 8.249512670565302,
      "grad_norm": 17.13652229309082,
      "learning_rate": 9.454545454545456e-06,
      "loss": 0.1908,
      "step": 1064
    },
    {
      "epoch": 8.265107212475634,
      "grad_norm": 26.64232635498047,
      "learning_rate": 9.434343434343435e-06,
      "loss": 0.1654,
      "step": 1066
    },
    {
      "epoch": 8.280701754385966,
      "grad_norm": 23.058507919311523,
      "learning_rate": 9.414141414141414e-06,
      "loss": 0.1454,
      "step": 1068
    },
    {
      "epoch": 8.296296296296296,
      "grad_norm": 30.088027954101562,
      "learning_rate": 9.393939393939396e-06,
      "loss": 0.1219,
      "step": 1070
    },
    {
      "epoch": 8.311890838206628,
      "grad_norm": 27.11796760559082,
      "learning_rate": 9.373737373737375e-06,
      "loss": 0.1592,
      "step": 1072
    },
    {
      "epoch": 8.32748538011696,
      "grad_norm": 21.083768844604492,
      "learning_rate": 9.353535353535354e-06,
      "loss": 0.1108,
      "step": 1074
    },
    {
      "epoch": 8.34307992202729,
      "grad_norm": 31.459501266479492,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.2077,
      "step": 1076
    },
    {
      "epoch": 8.358674463937621,
      "grad_norm": 26.995786666870117,
      "learning_rate": 9.313131313131313e-06,
      "loss": 0.241,
      "step": 1078
    },
    {
      "epoch": 8.374269005847953,
      "grad_norm": 29.296640396118164,
      "learning_rate": 9.292929292929294e-06,
      "loss": 0.2508,
      "step": 1080
    },
    {
      "epoch": 8.389863547758285,
      "grad_norm": 20.33731460571289,
      "learning_rate": 9.272727272727273e-06,
      "loss": 0.1215,
      "step": 1082
    },
    {
      "epoch": 8.405458089668617,
      "grad_norm": 18.956241607666016,
      "learning_rate": 9.252525252525253e-06,
      "loss": 0.1261,
      "step": 1084
    },
    {
      "epoch": 8.421052631578947,
      "grad_norm": 19.218971252441406,
      "learning_rate": 9.232323232323232e-06,
      "loss": 0.163,
      "step": 1086
    },
    {
      "epoch": 8.436647173489279,
      "grad_norm": 18.47032928466797,
      "learning_rate": 9.212121212121213e-06,
      "loss": 0.2985,
      "step": 1088
    },
    {
      "epoch": 8.45224171539961,
      "grad_norm": 21.898714065551758,
      "learning_rate": 9.191919191919193e-06,
      "loss": 0.1611,
      "step": 1090
    },
    {
      "epoch": 8.46783625730994,
      "grad_norm": 19.591819763183594,
      "learning_rate": 9.171717171717172e-06,
      "loss": 0.1164,
      "step": 1092
    },
    {
      "epoch": 8.483430799220272,
      "grad_norm": 26.901838302612305,
      "learning_rate": 9.151515151515153e-06,
      "loss": 0.1606,
      "step": 1094
    },
    {
      "epoch": 8.499025341130604,
      "grad_norm": 23.755897521972656,
      "learning_rate": 9.131313131313132e-06,
      "loss": 0.1378,
      "step": 1096
    },
    {
      "epoch": 8.514619883040936,
      "grad_norm": 21.365205764770508,
      "learning_rate": 9.111111111111112e-06,
      "loss": 0.1134,
      "step": 1098
    },
    {
      "epoch": 8.530214424951268,
      "grad_norm": 20.002904891967773,
      "learning_rate": 9.090909090909091e-06,
      "loss": 0.3189,
      "step": 1100
    },
    {
      "epoch": 8.545808966861598,
      "grad_norm": 26.882352828979492,
      "learning_rate": 9.070707070707072e-06,
      "loss": 0.1671,
      "step": 1102
    },
    {
      "epoch": 8.56140350877193,
      "grad_norm": 27.24085807800293,
      "learning_rate": 9.050505050505052e-06,
      "loss": 0.1267,
      "step": 1104
    },
    {
      "epoch": 8.576998050682262,
      "grad_norm": 18.332754135131836,
      "learning_rate": 9.030303030303031e-06,
      "loss": 0.1511,
      "step": 1106
    },
    {
      "epoch": 8.592592592592592,
      "grad_norm": 32.06694793701172,
      "learning_rate": 9.010101010101012e-06,
      "loss": 0.1622,
      "step": 1108
    },
    {
      "epoch": 8.608187134502923,
      "grad_norm": 41.591453552246094,
      "learning_rate": 8.98989898989899e-06,
      "loss": 0.2303,
      "step": 1110
    },
    {
      "epoch": 8.623781676413255,
      "grad_norm": 26.75042152404785,
      "learning_rate": 8.969696969696971e-06,
      "loss": 0.1818,
      "step": 1112
    },
    {
      "epoch": 8.639376218323587,
      "grad_norm": 29.376577377319336,
      "learning_rate": 8.94949494949495e-06,
      "loss": 0.2201,
      "step": 1114
    },
    {
      "epoch": 8.654970760233919,
      "grad_norm": 20.77351951599121,
      "learning_rate": 8.92929292929293e-06,
      "loss": 0.1571,
      "step": 1116
    },
    {
      "epoch": 8.670565302144249,
      "grad_norm": 16.116701126098633,
      "learning_rate": 8.90909090909091e-06,
      "loss": 0.2023,
      "step": 1118
    },
    {
      "epoch": 8.68615984405458,
      "grad_norm": 23.80711555480957,
      "learning_rate": 8.888888888888888e-06,
      "loss": 0.129,
      "step": 1120
    },
    {
      "epoch": 8.701754385964913,
      "grad_norm": 21.075159072875977,
      "learning_rate": 8.86868686868687e-06,
      "loss": 0.1994,
      "step": 1122
    },
    {
      "epoch": 8.717348927875243,
      "grad_norm": 27.987714767456055,
      "learning_rate": 8.84848484848485e-06,
      "loss": 0.2448,
      "step": 1124
    },
    {
      "epoch": 8.732943469785575,
      "grad_norm": 19.206315994262695,
      "learning_rate": 8.82828282828283e-06,
      "loss": 0.1427,
      "step": 1126
    },
    {
      "epoch": 8.748538011695906,
      "grad_norm": 30.6961612701416,
      "learning_rate": 8.808080808080809e-06,
      "loss": 0.2189,
      "step": 1128
    },
    {
      "epoch": 8.764132553606238,
      "grad_norm": 35.87345504760742,
      "learning_rate": 8.787878787878788e-06,
      "loss": 0.1757,
      "step": 1130
    },
    {
      "epoch": 8.77972709551657,
      "grad_norm": 20.79956817626953,
      "learning_rate": 8.767676767676768e-06,
      "loss": 0.1375,
      "step": 1132
    },
    {
      "epoch": 8.7953216374269,
      "grad_norm": 29.511003494262695,
      "learning_rate": 8.747474747474747e-06,
      "loss": 0.2369,
      "step": 1134
    },
    {
      "epoch": 8.810916179337232,
      "grad_norm": 23.777294158935547,
      "learning_rate": 8.727272727272728e-06,
      "loss": 0.1634,
      "step": 1136
    },
    {
      "epoch": 8.826510721247564,
      "grad_norm": 21.757246017456055,
      "learning_rate": 8.707070707070707e-06,
      "loss": 0.1298,
      "step": 1138
    },
    {
      "epoch": 8.842105263157894,
      "grad_norm": 18.296131134033203,
      "learning_rate": 8.686868686868687e-06,
      "loss": 0.1635,
      "step": 1140
    },
    {
      "epoch": 8.857699805068226,
      "grad_norm": 19.44378089904785,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.2267,
      "step": 1142
    },
    {
      "epoch": 8.873294346978557,
      "grad_norm": 20.93977165222168,
      "learning_rate": 8.646464646464647e-06,
      "loss": 0.2226,
      "step": 1144
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 18.78200912475586,
      "learning_rate": 8.626262626262627e-06,
      "loss": 0.1184,
      "step": 1146
    },
    {
      "epoch": 8.904483430799221,
      "grad_norm": 25.348186492919922,
      "learning_rate": 8.606060606060606e-06,
      "loss": 0.1939,
      "step": 1148
    },
    {
      "epoch": 8.920077972709551,
      "grad_norm": 42.17597579956055,
      "learning_rate": 8.585858585858587e-06,
      "loss": 0.2088,
      "step": 1150
    },
    {
      "epoch": 8.935672514619883,
      "grad_norm": 31.36518669128418,
      "learning_rate": 8.565656565656566e-06,
      "loss": 0.1523,
      "step": 1152
    },
    {
      "epoch": 8.951267056530215,
      "grad_norm": 30.081546783447266,
      "learning_rate": 8.545454545454546e-06,
      "loss": 0.1484,
      "step": 1154
    },
    {
      "epoch": 8.966861598440545,
      "grad_norm": 24.27243995666504,
      "learning_rate": 8.525252525252527e-06,
      "loss": 0.1539,
      "step": 1156
    },
    {
      "epoch": 8.982456140350877,
      "grad_norm": 26.651201248168945,
      "learning_rate": 8.505050505050506e-06,
      "loss": 0.2271,
      "step": 1158
    },
    {
      "epoch": 8.998050682261209,
      "grad_norm": 26.23269271850586,
      "learning_rate": 8.484848484848486e-06,
      "loss": 0.1731,
      "step": 1160
    },
    {
      "epoch": 9.007797270955166,
      "grad_norm": 15.647942543029785,
      "learning_rate": 8.464646464646465e-06,
      "loss": 0.0782,
      "step": 1162
    },
    {
      "epoch": 9.023391812865498,
      "grad_norm": 13.948593139648438,
      "learning_rate": 8.444444444444446e-06,
      "loss": 0.1785,
      "step": 1164
    },
    {
      "epoch": 9.038986354775828,
      "grad_norm": 18.53496742248535,
      "learning_rate": 8.424242424242425e-06,
      "loss": 0.1553,
      "step": 1166
    },
    {
      "epoch": 9.05458089668616,
      "grad_norm": 23.951366424560547,
      "learning_rate": 8.404040404040405e-06,
      "loss": 0.0828,
      "step": 1168
    },
    {
      "epoch": 9.070175438596491,
      "grad_norm": 23.383588790893555,
      "learning_rate": 8.383838383838384e-06,
      "loss": 0.2012,
      "step": 1170
    },
    {
      "epoch": 9.085769980506823,
      "grad_norm": 14.127076148986816,
      "learning_rate": 8.363636363636365e-06,
      "loss": 0.1225,
      "step": 1172
    },
    {
      "epoch": 9.101364522417153,
      "grad_norm": 25.056371688842773,
      "learning_rate": 8.343434343434345e-06,
      "loss": 0.1164,
      "step": 1174
    },
    {
      "epoch": 9.116959064327485,
      "grad_norm": 26.768205642700195,
      "learning_rate": 8.323232323232324e-06,
      "loss": 0.1813,
      "step": 1176
    },
    {
      "epoch": 9.132553606237817,
      "grad_norm": 18.290380477905273,
      "learning_rate": 8.303030303030305e-06,
      "loss": 0.1065,
      "step": 1178
    },
    {
      "epoch": 9.148148148148149,
      "grad_norm": 19.553068161010742,
      "learning_rate": 8.282828282828283e-06,
      "loss": 0.0787,
      "step": 1180
    },
    {
      "epoch": 9.163742690058479,
      "grad_norm": 24.044979095458984,
      "learning_rate": 8.262626262626264e-06,
      "loss": 0.1402,
      "step": 1182
    },
    {
      "epoch": 9.17933723196881,
      "grad_norm": 24.084667205810547,
      "learning_rate": 8.242424242424243e-06,
      "loss": 0.1387,
      "step": 1184
    },
    {
      "epoch": 9.194931773879143,
      "grad_norm": 21.462068557739258,
      "learning_rate": 8.222222222222222e-06,
      "loss": 0.1146,
      "step": 1186
    },
    {
      "epoch": 9.210526315789474,
      "grad_norm": 21.964876174926758,
      "learning_rate": 8.202020202020202e-06,
      "loss": 0.1309,
      "step": 1188
    },
    {
      "epoch": 9.226120857699804,
      "grad_norm": 27.859107971191406,
      "learning_rate": 8.181818181818183e-06,
      "loss": 0.1521,
      "step": 1190
    },
    {
      "epoch": 9.241715399610136,
      "grad_norm": 27.264768600463867,
      "learning_rate": 8.161616161616162e-06,
      "loss": 0.098,
      "step": 1192
    },
    {
      "epoch": 9.257309941520468,
      "grad_norm": 20.451656341552734,
      "learning_rate": 8.141414141414142e-06,
      "loss": 0.1308,
      "step": 1194
    },
    {
      "epoch": 9.2729044834308,
      "grad_norm": 22.208227157592773,
      "learning_rate": 8.121212121212121e-06,
      "loss": 0.1407,
      "step": 1196
    },
    {
      "epoch": 9.28849902534113,
      "grad_norm": 22.29307746887207,
      "learning_rate": 8.101010101010102e-06,
      "loss": 0.1098,
      "step": 1198
    },
    {
      "epoch": 9.304093567251462,
      "grad_norm": 23.247177124023438,
      "learning_rate": 8.08080808080808e-06,
      "loss": 0.1882,
      "step": 1200
    },
    {
      "epoch": 9.319688109161794,
      "grad_norm": 24.40445899963379,
      "learning_rate": 8.060606060606061e-06,
      "loss": 0.1359,
      "step": 1202
    },
    {
      "epoch": 9.335282651072125,
      "grad_norm": 13.32595157623291,
      "learning_rate": 8.04040404040404e-06,
      "loss": 0.1414,
      "step": 1204
    },
    {
      "epoch": 9.350877192982455,
      "grad_norm": 26.359703063964844,
      "learning_rate": 8.02020202020202e-06,
      "loss": 0.155,
      "step": 1206
    },
    {
      "epoch": 9.366471734892787,
      "grad_norm": 32.21303176879883,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.1769,
      "step": 1208
    },
    {
      "epoch": 9.382066276803119,
      "grad_norm": 29.588550567626953,
      "learning_rate": 7.97979797979798e-06,
      "loss": 0.1432,
      "step": 1210
    },
    {
      "epoch": 9.397660818713451,
      "grad_norm": 21.756603240966797,
      "learning_rate": 7.95959595959596e-06,
      "loss": 0.1373,
      "step": 1212
    },
    {
      "epoch": 9.413255360623781,
      "grad_norm": 28.819379806518555,
      "learning_rate": 7.93939393939394e-06,
      "loss": 0.1176,
      "step": 1214
    },
    {
      "epoch": 9.428849902534113,
      "grad_norm": 22.924524307250977,
      "learning_rate": 7.91919191919192e-06,
      "loss": 0.1394,
      "step": 1216
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 15.283197402954102,
      "learning_rate": 7.898989898989899e-06,
      "loss": 0.2012,
      "step": 1218
    },
    {
      "epoch": 9.460038986354776,
      "grad_norm": 36.33919906616211,
      "learning_rate": 7.87878787878788e-06,
      "loss": 0.1366,
      "step": 1220
    },
    {
      "epoch": 9.475633528265107,
      "grad_norm": 24.705333709716797,
      "learning_rate": 7.858585858585859e-06,
      "loss": 0.2264,
      "step": 1222
    },
    {
      "epoch": 9.491228070175438,
      "grad_norm": 22.99205207824707,
      "learning_rate": 7.838383838383839e-06,
      "loss": 0.1404,
      "step": 1224
    },
    {
      "epoch": 9.50682261208577,
      "grad_norm": 33.363346099853516,
      "learning_rate": 7.81818181818182e-06,
      "loss": 0.105,
      "step": 1226
    },
    {
      "epoch": 9.522417153996102,
      "grad_norm": 54.345462799072266,
      "learning_rate": 7.797979797979799e-06,
      "loss": 0.139,
      "step": 1228
    },
    {
      "epoch": 9.538011695906432,
      "grad_norm": 18.025590896606445,
      "learning_rate": 7.77777777777778e-06,
      "loss": 0.2054,
      "step": 1230
    },
    {
      "epoch": 9.553606237816764,
      "grad_norm": 16.448444366455078,
      "learning_rate": 7.757575757575758e-06,
      "loss": 0.1003,
      "step": 1232
    },
    {
      "epoch": 9.569200779727096,
      "grad_norm": 22.88336753845215,
      "learning_rate": 7.737373737373739e-06,
      "loss": 0.1419,
      "step": 1234
    },
    {
      "epoch": 9.584795321637428,
      "grad_norm": 22.01582145690918,
      "learning_rate": 7.717171717171717e-06,
      "loss": 0.2018,
      "step": 1236
    },
    {
      "epoch": 9.600389863547758,
      "grad_norm": 23.714868545532227,
      "learning_rate": 7.696969696969696e-06,
      "loss": 0.127,
      "step": 1238
    },
    {
      "epoch": 9.61598440545809,
      "grad_norm": 24.906110763549805,
      "learning_rate": 7.676767676767677e-06,
      "loss": 0.1465,
      "step": 1240
    },
    {
      "epoch": 9.631578947368421,
      "grad_norm": 22.340259552001953,
      "learning_rate": 7.656565656565658e-06,
      "loss": 0.1233,
      "step": 1242
    },
    {
      "epoch": 9.647173489278753,
      "grad_norm": 28.738130569458008,
      "learning_rate": 7.636363636363638e-06,
      "loss": 0.1787,
      "step": 1244
    },
    {
      "epoch": 9.662768031189083,
      "grad_norm": 35.810298919677734,
      "learning_rate": 7.616161616161617e-06,
      "loss": 0.1326,
      "step": 1246
    },
    {
      "epoch": 9.678362573099415,
      "grad_norm": 26.54306411743164,
      "learning_rate": 7.595959595959597e-06,
      "loss": 0.1662,
      "step": 1248
    },
    {
      "epoch": 9.693957115009747,
      "grad_norm": 16.161544799804688,
      "learning_rate": 7.5757575757575764e-06,
      "loss": 0.1364,
      "step": 1250
    },
    {
      "epoch": 9.709551656920079,
      "grad_norm": 25.615386962890625,
      "learning_rate": 7.555555555555556e-06,
      "loss": 0.1475,
      "step": 1252
    },
    {
      "epoch": 9.725146198830409,
      "grad_norm": 23.576791763305664,
      "learning_rate": 7.535353535353536e-06,
      "loss": 0.1268,
      "step": 1254
    },
    {
      "epoch": 9.74074074074074,
      "grad_norm": 22.589828491210938,
      "learning_rate": 7.515151515151516e-06,
      "loss": 0.1383,
      "step": 1256
    },
    {
      "epoch": 9.756335282651072,
      "grad_norm": 17.28984260559082,
      "learning_rate": 7.494949494949496e-06,
      "loss": 0.1558,
      "step": 1258
    },
    {
      "epoch": 9.771929824561404,
      "grad_norm": 18.8803768157959,
      "learning_rate": 7.474747474747476e-06,
      "loss": 0.2156,
      "step": 1260
    },
    {
      "epoch": 9.787524366471734,
      "grad_norm": 32.58103942871094,
      "learning_rate": 7.454545454545456e-06,
      "loss": 0.1357,
      "step": 1262
    },
    {
      "epoch": 9.803118908382066,
      "grad_norm": 26.02987289428711,
      "learning_rate": 7.434343434343435e-06,
      "loss": 0.1199,
      "step": 1264
    },
    {
      "epoch": 9.818713450292398,
      "grad_norm": 20.438899993896484,
      "learning_rate": 7.414141414141415e-06,
      "loss": 0.1333,
      "step": 1266
    },
    {
      "epoch": 9.83430799220273,
      "grad_norm": 30.661876678466797,
      "learning_rate": 7.393939393939395e-06,
      "loss": 0.1146,
      "step": 1268
    },
    {
      "epoch": 9.84990253411306,
      "grad_norm": 20.947914123535156,
      "learning_rate": 7.373737373737374e-06,
      "loss": 0.1047,
      "step": 1270
    },
    {
      "epoch": 9.865497076023392,
      "grad_norm": 20.56300926208496,
      "learning_rate": 7.353535353535353e-06,
      "loss": 0.1249,
      "step": 1272
    },
    {
      "epoch": 9.881091617933723,
      "grad_norm": 23.056781768798828,
      "learning_rate": 7.333333333333333e-06,
      "loss": 0.1383,
      "step": 1274
    },
    {
      "epoch": 9.896686159844055,
      "grad_norm": 16.065168380737305,
      "learning_rate": 7.3131313131313146e-06,
      "loss": 0.1839,
      "step": 1276
    },
    {
      "epoch": 9.912280701754385,
      "grad_norm": 30.676015853881836,
      "learning_rate": 7.2929292929292934e-06,
      "loss": 0.1776,
      "step": 1278
    },
    {
      "epoch": 9.927875243664717,
      "grad_norm": 23.812904357910156,
      "learning_rate": 7.272727272727273e-06,
      "loss": 0.1223,
      "step": 1280
    },
    {
      "epoch": 9.943469785575049,
      "grad_norm": 24.923847198486328,
      "learning_rate": 7.252525252525253e-06,
      "loss": 0.1713,
      "step": 1282
    },
    {
      "epoch": 9.95906432748538,
      "grad_norm": 20.243568420410156,
      "learning_rate": 7.232323232323233e-06,
      "loss": 0.1171,
      "step": 1284
    },
    {
      "epoch": 9.97465886939571,
      "grad_norm": 26.759092330932617,
      "learning_rate": 7.212121212121212e-06,
      "loss": 0.1368,
      "step": 1286
    },
    {
      "epoch": 9.990253411306043,
      "grad_norm": 29.440584182739258,
      "learning_rate": 7.191919191919192e-06,
      "loss": 0.1175,
      "step": 1288
    },
    {
      "epoch": 10.0,
      "grad_norm": 9.071653366088867,
      "learning_rate": 7.171717171717172e-06,
      "loss": 0.073,
      "step": 1290
    },
    {
      "epoch": 10.015594541910332,
      "grad_norm": 15.67029857635498,
      "learning_rate": 7.151515151515152e-06,
      "loss": 0.0845,
      "step": 1292
    },
    {
      "epoch": 10.031189083820664,
      "grad_norm": 15.425278663635254,
      "learning_rate": 7.131313131313132e-06,
      "loss": 0.1282,
      "step": 1294
    },
    {
      "epoch": 10.046783625730994,
      "grad_norm": 20.792091369628906,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.1006,
      "step": 1296
    },
    {
      "epoch": 10.062378167641326,
      "grad_norm": 23.36234474182129,
      "learning_rate": 7.0909090909090916e-06,
      "loss": 0.1245,
      "step": 1298
    },
    {
      "epoch": 10.077972709551657,
      "grad_norm": 18.15886878967285,
      "learning_rate": 7.070707070707071e-06,
      "loss": 0.0941,
      "step": 1300
    },
    {
      "epoch": 10.093567251461987,
      "grad_norm": 22.070354461669922,
      "learning_rate": 7.050505050505051e-06,
      "loss": 0.1067,
      "step": 1302
    },
    {
      "epoch": 10.10916179337232,
      "grad_norm": 20.826868057250977,
      "learning_rate": 7.030303030303031e-06,
      "loss": 0.1458,
      "step": 1304
    },
    {
      "epoch": 10.124756335282651,
      "grad_norm": 30.338909149169922,
      "learning_rate": 7.0101010101010105e-06,
      "loss": 0.126,
      "step": 1306
    },
    {
      "epoch": 10.140350877192983,
      "grad_norm": 18.80792236328125,
      "learning_rate": 6.98989898989899e-06,
      "loss": 0.0876,
      "step": 1308
    },
    {
      "epoch": 10.155945419103315,
      "grad_norm": 22.624279022216797,
      "learning_rate": 6.969696969696971e-06,
      "loss": 0.1676,
      "step": 1310
    },
    {
      "epoch": 10.171539961013645,
      "grad_norm": 20.169157028198242,
      "learning_rate": 6.9494949494949505e-06,
      "loss": 0.1009,
      "step": 1312
    },
    {
      "epoch": 10.187134502923977,
      "grad_norm": 13.032516479492188,
      "learning_rate": 6.92929292929293e-06,
      "loss": 0.0781,
      "step": 1314
    },
    {
      "epoch": 10.202729044834308,
      "grad_norm": 36.42633056640625,
      "learning_rate": 6.90909090909091e-06,
      "loss": 0.1071,
      "step": 1316
    },
    {
      "epoch": 10.218323586744638,
      "grad_norm": 16.01781463623047,
      "learning_rate": 6.88888888888889e-06,
      "loss": 0.0912,
      "step": 1318
    },
    {
      "epoch": 10.23391812865497,
      "grad_norm": 16.58261489868164,
      "learning_rate": 6.868686868686869e-06,
      "loss": 0.1615,
      "step": 1320
    },
    {
      "epoch": 10.249512670565302,
      "grad_norm": 24.54536247253418,
      "learning_rate": 6.848484848484849e-06,
      "loss": 0.1149,
      "step": 1322
    },
    {
      "epoch": 10.265107212475634,
      "grad_norm": 25.91512680053711,
      "learning_rate": 6.828282828282828e-06,
      "loss": 0.198,
      "step": 1324
    },
    {
      "epoch": 10.280701754385966,
      "grad_norm": 22.877582550048828,
      "learning_rate": 6.808080808080809e-06,
      "loss": 0.1235,
      "step": 1326
    },
    {
      "epoch": 10.296296296296296,
      "grad_norm": 19.751873016357422,
      "learning_rate": 6.787878787878789e-06,
      "loss": 0.1233,
      "step": 1328
    },
    {
      "epoch": 10.311890838206628,
      "grad_norm": 20.52838706970215,
      "learning_rate": 6.767676767676769e-06,
      "loss": 0.0936,
      "step": 1330
    },
    {
      "epoch": 10.32748538011696,
      "grad_norm": 28.20573616027832,
      "learning_rate": 6.747474747474749e-06,
      "loss": 0.1134,
      "step": 1332
    },
    {
      "epoch": 10.34307992202729,
      "grad_norm": 19.73306655883789,
      "learning_rate": 6.7272727272727275e-06,
      "loss": 0.0991,
      "step": 1334
    },
    {
      "epoch": 10.358674463937621,
      "grad_norm": 22.972742080688477,
      "learning_rate": 6.707070707070707e-06,
      "loss": 0.2557,
      "step": 1336
    },
    {
      "epoch": 10.374269005847953,
      "grad_norm": 23.277902603149414,
      "learning_rate": 6.686868686868687e-06,
      "loss": 0.094,
      "step": 1338
    },
    {
      "epoch": 10.389863547758285,
      "grad_norm": 25.188461303710938,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.131,
      "step": 1340
    },
    {
      "epoch": 10.405458089668617,
      "grad_norm": 25.05014419555664,
      "learning_rate": 6.646464646464646e-06,
      "loss": 0.1063,
      "step": 1342
    },
    {
      "epoch": 10.421052631578947,
      "grad_norm": 20.785877227783203,
      "learning_rate": 6.626262626262627e-06,
      "loss": 0.0921,
      "step": 1344
    },
    {
      "epoch": 10.436647173489279,
      "grad_norm": 25.953760147094727,
      "learning_rate": 6.606060606060607e-06,
      "loss": 0.1363,
      "step": 1346
    },
    {
      "epoch": 10.45224171539961,
      "grad_norm": 26.63947868347168,
      "learning_rate": 6.585858585858586e-06,
      "loss": 0.0961,
      "step": 1348
    },
    {
      "epoch": 10.46783625730994,
      "grad_norm": 16.929607391357422,
      "learning_rate": 6.565656565656566e-06,
      "loss": 0.1192,
      "step": 1350
    },
    {
      "epoch": 10.483430799220272,
      "grad_norm": 25.624618530273438,
      "learning_rate": 6.545454545454546e-06,
      "loss": 0.0979,
      "step": 1352
    },
    {
      "epoch": 10.499025341130604,
      "grad_norm": 16.84709358215332,
      "learning_rate": 6.525252525252526e-06,
      "loss": 0.1387,
      "step": 1354
    },
    {
      "epoch": 10.514619883040936,
      "grad_norm": 20.592039108276367,
      "learning_rate": 6.505050505050505e-06,
      "loss": 0.1159,
      "step": 1356
    },
    {
      "epoch": 10.530214424951268,
      "grad_norm": 23.044809341430664,
      "learning_rate": 6.484848484848485e-06,
      "loss": 0.1121,
      "step": 1358
    },
    {
      "epoch": 10.545808966861598,
      "grad_norm": 23.17406463623047,
      "learning_rate": 6.464646464646466e-06,
      "loss": 0.1666,
      "step": 1360
    },
    {
      "epoch": 10.56140350877193,
      "grad_norm": 17.431371688842773,
      "learning_rate": 6.444444444444445e-06,
      "loss": 0.1139,
      "step": 1362
    },
    {
      "epoch": 10.576998050682262,
      "grad_norm": 18.51362419128418,
      "learning_rate": 6.424242424242425e-06,
      "loss": 0.1585,
      "step": 1364
    },
    {
      "epoch": 10.592592592592592,
      "grad_norm": 22.42557144165039,
      "learning_rate": 6.404040404040405e-06,
      "loss": 0.1061,
      "step": 1366
    },
    {
      "epoch": 10.608187134502923,
      "grad_norm": 16.2473201751709,
      "learning_rate": 6.3838383838383845e-06,
      "loss": 0.0902,
      "step": 1368
    },
    {
      "epoch": 10.623781676413255,
      "grad_norm": 22.4327392578125,
      "learning_rate": 6.363636363636364e-06,
      "loss": 0.1182,
      "step": 1370
    },
    {
      "epoch": 10.639376218323587,
      "grad_norm": 25.74833869934082,
      "learning_rate": 6.343434343434344e-06,
      "loss": 0.0958,
      "step": 1372
    },
    {
      "epoch": 10.654970760233919,
      "grad_norm": 20.99152946472168,
      "learning_rate": 6.323232323232324e-06,
      "loss": 0.1486,
      "step": 1374
    },
    {
      "epoch": 10.670565302144249,
      "grad_norm": 28.0266056060791,
      "learning_rate": 6.303030303030303e-06,
      "loss": 0.1748,
      "step": 1376
    },
    {
      "epoch": 10.68615984405458,
      "grad_norm": 77.29312133789062,
      "learning_rate": 6.282828282828284e-06,
      "loss": 0.1391,
      "step": 1378
    },
    {
      "epoch": 10.701754385964913,
      "grad_norm": 18.172744750976562,
      "learning_rate": 6.262626262626264e-06,
      "loss": 0.1458,
      "step": 1380
    },
    {
      "epoch": 10.717348927875243,
      "grad_norm": 19.156742095947266,
      "learning_rate": 6.2424242424242434e-06,
      "loss": 0.1323,
      "step": 1382
    },
    {
      "epoch": 10.732943469785575,
      "grad_norm": 17.66765594482422,
      "learning_rate": 6.222222222222223e-06,
      "loss": 0.1152,
      "step": 1384
    },
    {
      "epoch": 10.748538011695906,
      "grad_norm": 18.227317810058594,
      "learning_rate": 6.202020202020203e-06,
      "loss": 0.0925,
      "step": 1386
    },
    {
      "epoch": 10.764132553606238,
      "grad_norm": 15.719212532043457,
      "learning_rate": 6.181818181818182e-06,
      "loss": 0.0793,
      "step": 1388
    },
    {
      "epoch": 10.77972709551657,
      "grad_norm": 18.943628311157227,
      "learning_rate": 6.1616161616161615e-06,
      "loss": 0.103,
      "step": 1390
    },
    {
      "epoch": 10.7953216374269,
      "grad_norm": 19.512187957763672,
      "learning_rate": 6.141414141414141e-06,
      "loss": 0.1843,
      "step": 1392
    },
    {
      "epoch": 10.810916179337232,
      "grad_norm": 17.398128509521484,
      "learning_rate": 6.121212121212121e-06,
      "loss": 0.1047,
      "step": 1394
    },
    {
      "epoch": 10.826510721247564,
      "grad_norm": 19.232521057128906,
      "learning_rate": 6.1010101010101015e-06,
      "loss": 0.1196,
      "step": 1396
    },
    {
      "epoch": 10.842105263157894,
      "grad_norm": 22.935958862304688,
      "learning_rate": 6.080808080808081e-06,
      "loss": 0.1474,
      "step": 1398
    },
    {
      "epoch": 10.857699805068226,
      "grad_norm": 12.26579761505127,
      "learning_rate": 6.060606060606061e-06,
      "loss": 0.0762,
      "step": 1400
    },
    {
      "epoch": 10.873294346978557,
      "grad_norm": 21.631019592285156,
      "learning_rate": 6.040404040404041e-06,
      "loss": 0.0829,
      "step": 1402
    },
    {
      "epoch": 10.88888888888889,
      "grad_norm": 20.36859130859375,
      "learning_rate": 6.0202020202020204e-06,
      "loss": 0.1257,
      "step": 1404
    },
    {
      "epoch": 10.904483430799221,
      "grad_norm": 25.577852249145508,
      "learning_rate": 6e-06,
      "loss": 0.0973,
      "step": 1406
    },
    {
      "epoch": 10.920077972709551,
      "grad_norm": 31.974655151367188,
      "learning_rate": 5.97979797979798e-06,
      "loss": 0.134,
      "step": 1408
    },
    {
      "epoch": 10.935672514619883,
      "grad_norm": 24.523929595947266,
      "learning_rate": 5.95959595959596e-06,
      "loss": 0.0954,
      "step": 1410
    },
    {
      "epoch": 10.951267056530215,
      "grad_norm": 17.68484878540039,
      "learning_rate": 5.93939393939394e-06,
      "loss": 0.1617,
      "step": 1412
    },
    {
      "epoch": 10.966861598440545,
      "grad_norm": 20.591859817504883,
      "learning_rate": 5.91919191919192e-06,
      "loss": 0.0932,
      "step": 1414
    },
    {
      "epoch": 10.982456140350877,
      "grad_norm": 32.513301849365234,
      "learning_rate": 5.8989898989899e-06,
      "loss": 0.1122,
      "step": 1416
    },
    {
      "epoch": 10.998050682261209,
      "grad_norm": 17.55221176147461,
      "learning_rate": 5.878787878787879e-06,
      "loss": 0.0834,
      "step": 1418
    },
    {
      "epoch": 11.007797270955166,
      "grad_norm": 14.29137897491455,
      "learning_rate": 5.858585858585859e-06,
      "loss": 0.0502,
      "step": 1420
    },
    {
      "epoch": 11.023391812865498,
      "grad_norm": 16.377592086791992,
      "learning_rate": 5.838383838383839e-06,
      "loss": 0.0864,
      "step": 1422
    },
    {
      "epoch": 11.038986354775828,
      "grad_norm": 14.947640419006348,
      "learning_rate": 5.8181818181818185e-06,
      "loss": 0.1077,
      "step": 1424
    },
    {
      "epoch": 11.05458089668616,
      "grad_norm": 17.656057357788086,
      "learning_rate": 5.797979797979798e-06,
      "loss": 0.0932,
      "step": 1426
    },
    {
      "epoch": 11.070175438596491,
      "grad_norm": 14.82080078125,
      "learning_rate": 5.777777777777778e-06,
      "loss": 0.1023,
      "step": 1428
    },
    {
      "epoch": 11.085769980506823,
      "grad_norm": 19.42533302307129,
      "learning_rate": 5.7575757575757586e-06,
      "loss": 0.0821,
      "step": 1430
    },
    {
      "epoch": 11.101364522417153,
      "grad_norm": 8.989033699035645,
      "learning_rate": 5.737373737373738e-06,
      "loss": 0.0667,
      "step": 1432
    },
    {
      "epoch": 11.116959064327485,
      "grad_norm": 14.880553245544434,
      "learning_rate": 5.717171717171718e-06,
      "loss": 0.0639,
      "step": 1434
    },
    {
      "epoch": 11.132553606237817,
      "grad_norm": 30.362903594970703,
      "learning_rate": 5.696969696969698e-06,
      "loss": 0.0843,
      "step": 1436
    },
    {
      "epoch": 11.148148148148149,
      "grad_norm": 30.81125831604004,
      "learning_rate": 5.6767676767676775e-06,
      "loss": 0.1345,
      "step": 1438
    },
    {
      "epoch": 11.163742690058479,
      "grad_norm": 39.39702224731445,
      "learning_rate": 5.656565656565657e-06,
      "loss": 0.0928,
      "step": 1440
    },
    {
      "epoch": 11.17933723196881,
      "grad_norm": 17.25457000732422,
      "learning_rate": 5.636363636363636e-06,
      "loss": 0.07,
      "step": 1442
    },
    {
      "epoch": 11.194931773879143,
      "grad_norm": 20.110713958740234,
      "learning_rate": 5.616161616161616e-06,
      "loss": 0.1262,
      "step": 1444
    },
    {
      "epoch": 11.210526315789474,
      "grad_norm": 19.110309600830078,
      "learning_rate": 5.595959595959597e-06,
      "loss": 0.1331,
      "step": 1446
    },
    {
      "epoch": 11.226120857699804,
      "grad_norm": 17.10846710205078,
      "learning_rate": 5.575757575757577e-06,
      "loss": 0.0723,
      "step": 1448
    },
    {
      "epoch": 11.241715399610136,
      "grad_norm": 12.672163009643555,
      "learning_rate": 5.555555555555557e-06,
      "loss": 0.1478,
      "step": 1450
    },
    {
      "epoch": 11.257309941520468,
      "grad_norm": 15.096246719360352,
      "learning_rate": 5.5353535353535355e-06,
      "loss": 0.0772,
      "step": 1452
    },
    {
      "epoch": 11.2729044834308,
      "grad_norm": 18.2756404876709,
      "learning_rate": 5.515151515151515e-06,
      "loss": 0.1669,
      "step": 1454
    },
    {
      "epoch": 11.28849902534113,
      "grad_norm": 17.41407585144043,
      "learning_rate": 5.494949494949495e-06,
      "loss": 0.074,
      "step": 1456
    },
    {
      "epoch": 11.304093567251462,
      "grad_norm": 13.701067924499512,
      "learning_rate": 5.474747474747475e-06,
      "loss": 0.0811,
      "step": 1458
    },
    {
      "epoch": 11.319688109161794,
      "grad_norm": 8.650382041931152,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 0.0562,
      "step": 1460
    },
    {
      "epoch": 11.335282651072125,
      "grad_norm": 18.39655876159668,
      "learning_rate": 5.434343434343434e-06,
      "loss": 0.1497,
      "step": 1462
    },
    {
      "epoch": 11.350877192982455,
      "grad_norm": 17.569480895996094,
      "learning_rate": 5.414141414141415e-06,
      "loss": 0.0774,
      "step": 1464
    },
    {
      "epoch": 11.366471734892787,
      "grad_norm": 18.6324520111084,
      "learning_rate": 5.3939393939393945e-06,
      "loss": 0.068,
      "step": 1466
    },
    {
      "epoch": 11.382066276803119,
      "grad_norm": 15.796083450317383,
      "learning_rate": 5.373737373737374e-06,
      "loss": 0.084,
      "step": 1468
    },
    {
      "epoch": 11.397660818713451,
      "grad_norm": 19.46818733215332,
      "learning_rate": 5.353535353535354e-06,
      "loss": 0.0949,
      "step": 1470
    },
    {
      "epoch": 11.413255360623781,
      "grad_norm": 22.920320510864258,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.1198,
      "step": 1472
    },
    {
      "epoch": 11.428849902534113,
      "grad_norm": 15.859986305236816,
      "learning_rate": 5.313131313131313e-06,
      "loss": 0.0835,
      "step": 1474
    },
    {
      "epoch": 11.444444444444445,
      "grad_norm": 19.407176971435547,
      "learning_rate": 5.292929292929293e-06,
      "loss": 0.1374,
      "step": 1476
    },
    {
      "epoch": 11.460038986354776,
      "grad_norm": 29.597000122070312,
      "learning_rate": 5.272727272727273e-06,
      "loss": 0.1161,
      "step": 1478
    },
    {
      "epoch": 11.475633528265107,
      "grad_norm": 19.667264938354492,
      "learning_rate": 5.252525252525253e-06,
      "loss": 0.0824,
      "step": 1480
    },
    {
      "epoch": 11.491228070175438,
      "grad_norm": 19.00667953491211,
      "learning_rate": 5.232323232323233e-06,
      "loss": 0.1155,
      "step": 1482
    },
    {
      "epoch": 11.50682261208577,
      "grad_norm": 15.377081871032715,
      "learning_rate": 5.212121212121213e-06,
      "loss": 0.0921,
      "step": 1484
    },
    {
      "epoch": 11.522417153996102,
      "grad_norm": 19.435537338256836,
      "learning_rate": 5.191919191919193e-06,
      "loss": 0.0805,
      "step": 1486
    },
    {
      "epoch": 11.538011695906432,
      "grad_norm": 30.99613380432129,
      "learning_rate": 5.171717171717172e-06,
      "loss": 0.0897,
      "step": 1488
    },
    {
      "epoch": 11.553606237816764,
      "grad_norm": 18.448516845703125,
      "learning_rate": 5.151515151515152e-06,
      "loss": 0.1147,
      "step": 1490
    },
    {
      "epoch": 11.569200779727096,
      "grad_norm": 15.903013229370117,
      "learning_rate": 5.131313131313132e-06,
      "loss": 0.0741,
      "step": 1492
    },
    {
      "epoch": 11.584795321637428,
      "grad_norm": 13.698027610778809,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 0.0962,
      "step": 1494
    },
    {
      "epoch": 11.600389863547758,
      "grad_norm": 19.752519607543945,
      "learning_rate": 5.090909090909091e-06,
      "loss": 0.0819,
      "step": 1496
    },
    {
      "epoch": 11.61598440545809,
      "grad_norm": 17.928274154663086,
      "learning_rate": 5.070707070707072e-06,
      "loss": 0.1278,
      "step": 1498
    },
    {
      "epoch": 11.631578947368421,
      "grad_norm": 17.1124210357666,
      "learning_rate": 5.0505050505050515e-06,
      "loss": 0.0841,
      "step": 1500
    },
    {
      "epoch": 11.647173489278753,
      "grad_norm": 21.03636360168457,
      "learning_rate": 5.030303030303031e-06,
      "loss": 0.0757,
      "step": 1502
    },
    {
      "epoch": 11.662768031189083,
      "grad_norm": 16.629802703857422,
      "learning_rate": 5.010101010101011e-06,
      "loss": 0.1103,
      "step": 1504
    },
    {
      "epoch": 11.678362573099415,
      "grad_norm": 14.992215156555176,
      "learning_rate": 4.98989898989899e-06,
      "loss": 0.0811,
      "step": 1506
    },
    {
      "epoch": 11.693957115009747,
      "grad_norm": 20.118080139160156,
      "learning_rate": 4.9696969696969696e-06,
      "loss": 0.0809,
      "step": 1508
    },
    {
      "epoch": 11.709551656920079,
      "grad_norm": 20.187711715698242,
      "learning_rate": 4.94949494949495e-06,
      "loss": 0.1268,
      "step": 1510
    },
    {
      "epoch": 11.725146198830409,
      "grad_norm": 17.54914665222168,
      "learning_rate": 4.92929292929293e-06,
      "loss": 0.1168,
      "step": 1512
    },
    {
      "epoch": 11.74074074074074,
      "grad_norm": 23.752717971801758,
      "learning_rate": 4.90909090909091e-06,
      "loss": 0.1735,
      "step": 1514
    },
    {
      "epoch": 11.756335282651072,
      "grad_norm": 21.573801040649414,
      "learning_rate": 4.888888888888889e-06,
      "loss": 0.0855,
      "step": 1516
    },
    {
      "epoch": 11.771929824561404,
      "grad_norm": 15.937041282653809,
      "learning_rate": 4.868686868686869e-06,
      "loss": 0.1976,
      "step": 1518
    },
    {
      "epoch": 11.787524366471734,
      "grad_norm": 19.955415725708008,
      "learning_rate": 4.848484848484849e-06,
      "loss": 0.1205,
      "step": 1520
    },
    {
      "epoch": 11.803118908382066,
      "grad_norm": 21.587472915649414,
      "learning_rate": 4.8282828282828285e-06,
      "loss": 0.1099,
      "step": 1522
    },
    {
      "epoch": 11.818713450292398,
      "grad_norm": 19.186485290527344,
      "learning_rate": 4.808080808080808e-06,
      "loss": 0.088,
      "step": 1524
    },
    {
      "epoch": 11.83430799220273,
      "grad_norm": 12.616744995117188,
      "learning_rate": 4.787878787878788e-06,
      "loss": 0.0804,
      "step": 1526
    },
    {
      "epoch": 11.84990253411306,
      "grad_norm": 23.38275909423828,
      "learning_rate": 4.7676767676767685e-06,
      "loss": 0.1732,
      "step": 1528
    },
    {
      "epoch": 11.865497076023392,
      "grad_norm": 24.54665184020996,
      "learning_rate": 4.747474747474748e-06,
      "loss": 0.1442,
      "step": 1530
    },
    {
      "epoch": 11.881091617933723,
      "grad_norm": 17.815502166748047,
      "learning_rate": 4.727272727272728e-06,
      "loss": 0.1007,
      "step": 1532
    },
    {
      "epoch": 11.896686159844055,
      "grad_norm": 20.18832015991211,
      "learning_rate": 4.707070707070707e-06,
      "loss": 0.1105,
      "step": 1534
    },
    {
      "epoch": 11.912280701754385,
      "grad_norm": 15.933636665344238,
      "learning_rate": 4.6868686868686874e-06,
      "loss": 0.1024,
      "step": 1536
    },
    {
      "epoch": 11.927875243664717,
      "grad_norm": 18.771953582763672,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.1146,
      "step": 1538
    },
    {
      "epoch": 11.943469785575049,
      "grad_norm": 14.276867866516113,
      "learning_rate": 4.646464646464647e-06,
      "loss": 0.1281,
      "step": 1540
    },
    {
      "epoch": 11.95906432748538,
      "grad_norm": 21.85449981689453,
      "learning_rate": 4.626262626262627e-06,
      "loss": 0.1175,
      "step": 1542
    },
    {
      "epoch": 11.97465886939571,
      "grad_norm": 21.888336181640625,
      "learning_rate": 4.606060606060606e-06,
      "loss": 0.1085,
      "step": 1544
    },
    {
      "epoch": 11.990253411306043,
      "grad_norm": 17.418615341186523,
      "learning_rate": 4.585858585858586e-06,
      "loss": 0.0979,
      "step": 1546
    },
    {
      "epoch": 12.0,
      "grad_norm": 9.940718650817871,
      "learning_rate": 4.565656565656566e-06,
      "loss": 0.0449,
      "step": 1548
    },
    {
      "epoch": 12.015594541910332,
      "grad_norm": 10.130072593688965,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 0.0739,
      "step": 1550
    },
    {
      "epoch": 12.031189083820664,
      "grad_norm": 15.928557395935059,
      "learning_rate": 4.525252525252526e-06,
      "loss": 0.1093,
      "step": 1552
    },
    {
      "epoch": 12.046783625730994,
      "grad_norm": 13.26473617553711,
      "learning_rate": 4.505050505050506e-06,
      "loss": 0.0945,
      "step": 1554
    },
    {
      "epoch": 12.062378167641326,
      "grad_norm": 17.515823364257812,
      "learning_rate": 4.4848484848484855e-06,
      "loss": 0.103,
      "step": 1556
    },
    {
      "epoch": 12.077972709551657,
      "grad_norm": 14.4723482131958,
      "learning_rate": 4.464646464646465e-06,
      "loss": 0.063,
      "step": 1558
    },
    {
      "epoch": 12.093567251461987,
      "grad_norm": 17.603225708007812,
      "learning_rate": 4.444444444444444e-06,
      "loss": 0.0727,
      "step": 1560
    },
    {
      "epoch": 12.10916179337232,
      "grad_norm": 17.125280380249023,
      "learning_rate": 4.424242424242425e-06,
      "loss": 0.0801,
      "step": 1562
    },
    {
      "epoch": 12.124756335282651,
      "grad_norm": 14.704657554626465,
      "learning_rate": 4.4040404040404044e-06,
      "loss": 0.0718,
      "step": 1564
    },
    {
      "epoch": 12.140350877192983,
      "grad_norm": 20.472501754760742,
      "learning_rate": 4.383838383838384e-06,
      "loss": 0.0803,
      "step": 1566
    },
    {
      "epoch": 12.155945419103315,
      "grad_norm": 16.212158203125,
      "learning_rate": 4.363636363636364e-06,
      "loss": 0.0713,
      "step": 1568
    },
    {
      "epoch": 12.171539961013645,
      "grad_norm": 17.29305076599121,
      "learning_rate": 4.343434343434344e-06,
      "loss": 0.0709,
      "step": 1570
    },
    {
      "epoch": 12.187134502923977,
      "grad_norm": 21.901208877563477,
      "learning_rate": 4.323232323232323e-06,
      "loss": 0.0716,
      "step": 1572
    },
    {
      "epoch": 12.202729044834308,
      "grad_norm": 11.552567481994629,
      "learning_rate": 4.303030303030303e-06,
      "loss": 0.0688,
      "step": 1574
    },
    {
      "epoch": 12.218323586744638,
      "grad_norm": 65.15922546386719,
      "learning_rate": 4.282828282828283e-06,
      "loss": 0.0732,
      "step": 1576
    },
    {
      "epoch": 12.23391812865497,
      "grad_norm": 18.223798751831055,
      "learning_rate": 4.262626262626263e-06,
      "loss": 0.0727,
      "step": 1578
    },
    {
      "epoch": 12.249512670565302,
      "grad_norm": 16.354141235351562,
      "learning_rate": 4.242424242424243e-06,
      "loss": 0.076,
      "step": 1580
    },
    {
      "epoch": 12.265107212475634,
      "grad_norm": 15.543667793273926,
      "learning_rate": 4.222222222222223e-06,
      "loss": 0.0756,
      "step": 1582
    },
    {
      "epoch": 12.280701754385966,
      "grad_norm": 17.1079158782959,
      "learning_rate": 4.2020202020202026e-06,
      "loss": 0.0774,
      "step": 1584
    },
    {
      "epoch": 12.296296296296296,
      "grad_norm": 21.139108657836914,
      "learning_rate": 4.181818181818182e-06,
      "loss": 0.1577,
      "step": 1586
    },
    {
      "epoch": 12.311890838206628,
      "grad_norm": 13.714010238647461,
      "learning_rate": 4.161616161616162e-06,
      "loss": 0.1642,
      "step": 1588
    },
    {
      "epoch": 12.32748538011696,
      "grad_norm": 14.13723087310791,
      "learning_rate": 4.141414141414142e-06,
      "loss": 0.1324,
      "step": 1590
    },
    {
      "epoch": 12.34307992202729,
      "grad_norm": 23.049480438232422,
      "learning_rate": 4.1212121212121215e-06,
      "loss": 0.0725,
      "step": 1592
    },
    {
      "epoch": 12.358674463937621,
      "grad_norm": 12.682608604431152,
      "learning_rate": 4.101010101010101e-06,
      "loss": 0.0695,
      "step": 1594
    },
    {
      "epoch": 12.374269005847953,
      "grad_norm": 16.23015785217285,
      "learning_rate": 4.080808080808081e-06,
      "loss": 0.1055,
      "step": 1596
    },
    {
      "epoch": 12.389863547758285,
      "grad_norm": 16.899337768554688,
      "learning_rate": 4.060606060606061e-06,
      "loss": 0.0804,
      "step": 1598
    },
    {
      "epoch": 12.405458089668617,
      "grad_norm": 17.99395751953125,
      "learning_rate": 4.04040404040404e-06,
      "loss": 0.0813,
      "step": 1600
    },
    {
      "epoch": 12.421052631578947,
      "grad_norm": 19.77749252319336,
      "learning_rate": 4.02020202020202e-06,
      "loss": 0.0913,
      "step": 1602
    },
    {
      "epoch": 12.436647173489279,
      "grad_norm": 11.026233673095703,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0845,
      "step": 1604
    },
    {
      "epoch": 12.45224171539961,
      "grad_norm": 12.651773452758789,
      "learning_rate": 3.97979797979798e-06,
      "loss": 0.0673,
      "step": 1606
    },
    {
      "epoch": 12.46783625730994,
      "grad_norm": 18.340641021728516,
      "learning_rate": 3.95959595959596e-06,
      "loss": 0.1002,
      "step": 1608
    },
    {
      "epoch": 12.483430799220272,
      "grad_norm": 13.070516586303711,
      "learning_rate": 3.93939393939394e-06,
      "loss": 0.0691,
      "step": 1610
    },
    {
      "epoch": 12.499025341130604,
      "grad_norm": 9.97529411315918,
      "learning_rate": 3.9191919191919196e-06,
      "loss": 0.0676,
      "step": 1612
    },
    {
      "epoch": 12.514619883040936,
      "grad_norm": 17.871736526489258,
      "learning_rate": 3.898989898989899e-06,
      "loss": 0.1115,
      "step": 1614
    },
    {
      "epoch": 12.530214424951268,
      "grad_norm": 10.155363082885742,
      "learning_rate": 3.878787878787879e-06,
      "loss": 0.0792,
      "step": 1616
    },
    {
      "epoch": 12.545808966861598,
      "grad_norm": 18.587326049804688,
      "learning_rate": 3.858585858585859e-06,
      "loss": 0.0887,
      "step": 1618
    },
    {
      "epoch": 12.56140350877193,
      "grad_norm": 16.86262321472168,
      "learning_rate": 3.8383838383838385e-06,
      "loss": 0.0805,
      "step": 1620
    },
    {
      "epoch": 12.576998050682262,
      "grad_norm": 16.62896156311035,
      "learning_rate": 3.818181818181819e-06,
      "loss": 0.0727,
      "step": 1622
    },
    {
      "epoch": 12.592592592592592,
      "grad_norm": 22.998825073242188,
      "learning_rate": 3.7979797979797984e-06,
      "loss": 0.0865,
      "step": 1624
    },
    {
      "epoch": 12.608187134502923,
      "grad_norm": 14.582930564880371,
      "learning_rate": 3.777777777777778e-06,
      "loss": 0.0748,
      "step": 1626
    },
    {
      "epoch": 12.623781676413255,
      "grad_norm": 18.100751876831055,
      "learning_rate": 3.757575757575758e-06,
      "loss": 0.1357,
      "step": 1628
    },
    {
      "epoch": 12.639376218323587,
      "grad_norm": 30.583065032958984,
      "learning_rate": 3.737373737373738e-06,
      "loss": 0.0723,
      "step": 1630
    },
    {
      "epoch": 12.654970760233919,
      "grad_norm": 18.12589454650879,
      "learning_rate": 3.7171717171717177e-06,
      "loss": 0.0831,
      "step": 1632
    },
    {
      "epoch": 12.670565302144249,
      "grad_norm": 14.650609970092773,
      "learning_rate": 3.6969696969696974e-06,
      "loss": 0.119,
      "step": 1634
    },
    {
      "epoch": 12.68615984405458,
      "grad_norm": 20.226333618164062,
      "learning_rate": 3.6767676767676767e-06,
      "loss": 0.1084,
      "step": 1636
    },
    {
      "epoch": 12.701754385964913,
      "grad_norm": 37.173274993896484,
      "learning_rate": 3.6565656565656573e-06,
      "loss": 0.0767,
      "step": 1638
    },
    {
      "epoch": 12.717348927875243,
      "grad_norm": 15.292328834533691,
      "learning_rate": 3.6363636363636366e-06,
      "loss": 0.1074,
      "step": 1640
    },
    {
      "epoch": 12.732943469785575,
      "grad_norm": 22.059261322021484,
      "learning_rate": 3.6161616161616163e-06,
      "loss": 0.1002,
      "step": 1642
    },
    {
      "epoch": 12.748538011695906,
      "grad_norm": 11.489568710327148,
      "learning_rate": 3.595959595959596e-06,
      "loss": 0.0766,
      "step": 1644
    },
    {
      "epoch": 12.764132553606238,
      "grad_norm": 14.15176773071289,
      "learning_rate": 3.575757575757576e-06,
      "loss": 0.0879,
      "step": 1646
    },
    {
      "epoch": 12.77972709551657,
      "grad_norm": 12.858147621154785,
      "learning_rate": 3.555555555555556e-06,
      "loss": 0.0885,
      "step": 1648
    },
    {
      "epoch": 12.7953216374269,
      "grad_norm": 23.128820419311523,
      "learning_rate": 3.5353535353535356e-06,
      "loss": 0.1341,
      "step": 1650
    },
    {
      "epoch": 12.810916179337232,
      "grad_norm": 18.746932983398438,
      "learning_rate": 3.5151515151515154e-06,
      "loss": 0.0956,
      "step": 1652
    },
    {
      "epoch": 12.826510721247564,
      "grad_norm": 15.166552543640137,
      "learning_rate": 3.494949494949495e-06,
      "loss": 0.1171,
      "step": 1654
    },
    {
      "epoch": 12.842105263157894,
      "grad_norm": 11.843535423278809,
      "learning_rate": 3.4747474747474752e-06,
      "loss": 0.0627,
      "step": 1656
    },
    {
      "epoch": 12.857699805068226,
      "grad_norm": 35.629150390625,
      "learning_rate": 3.454545454545455e-06,
      "loss": 0.08,
      "step": 1658
    },
    {
      "epoch": 12.873294346978557,
      "grad_norm": 15.536934852600098,
      "learning_rate": 3.4343434343434347e-06,
      "loss": 0.1589,
      "step": 1660
    },
    {
      "epoch": 12.88888888888889,
      "grad_norm": 19.56536102294922,
      "learning_rate": 3.414141414141414e-06,
      "loss": 0.0744,
      "step": 1662
    },
    {
      "epoch": 12.904483430799221,
      "grad_norm": 16.986433029174805,
      "learning_rate": 3.3939393939393946e-06,
      "loss": 0.1043,
      "step": 1664
    },
    {
      "epoch": 12.920077972709551,
      "grad_norm": 17.283315658569336,
      "learning_rate": 3.3737373737373743e-06,
      "loss": 0.1143,
      "step": 1666
    },
    {
      "epoch": 12.935672514619883,
      "grad_norm": 21.28139305114746,
      "learning_rate": 3.3535353535353536e-06,
      "loss": 0.1385,
      "step": 1668
    },
    {
      "epoch": 12.951267056530215,
      "grad_norm": 17.9874210357666,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0818,
      "step": 1670
    },
    {
      "epoch": 12.966861598440545,
      "grad_norm": 19.582979202270508,
      "learning_rate": 3.3131313131313135e-06,
      "loss": 0.1067,
      "step": 1672
    },
    {
      "epoch": 12.982456140350877,
      "grad_norm": 15.346957206726074,
      "learning_rate": 3.292929292929293e-06,
      "loss": 0.072,
      "step": 1674
    },
    {
      "epoch": 12.998050682261209,
      "grad_norm": 17.13605308532715,
      "learning_rate": 3.272727272727273e-06,
      "loss": 0.0909,
      "step": 1676
    },
    {
      "epoch": 13.007797270955166,
      "grad_norm": 10.328311920166016,
      "learning_rate": 3.2525252525252527e-06,
      "loss": 0.0354,
      "step": 1678
    },
    {
      "epoch": 13.023391812865498,
      "grad_norm": 9.591788291931152,
      "learning_rate": 3.232323232323233e-06,
      "loss": 0.0994,
      "step": 1680
    },
    {
      "epoch": 13.038986354775828,
      "grad_norm": 10.426928520202637,
      "learning_rate": 3.2121212121212125e-06,
      "loss": 0.0878,
      "step": 1682
    },
    {
      "epoch": 13.05458089668616,
      "grad_norm": 11.69320297241211,
      "learning_rate": 3.1919191919191923e-06,
      "loss": 0.0732,
      "step": 1684
    },
    {
      "epoch": 13.070175438596491,
      "grad_norm": 16.280807495117188,
      "learning_rate": 3.171717171717172e-06,
      "loss": 0.0709,
      "step": 1686
    },
    {
      "epoch": 13.085769980506823,
      "grad_norm": 12.029870986938477,
      "learning_rate": 3.1515151515151517e-06,
      "loss": 0.0661,
      "step": 1688
    },
    {
      "epoch": 13.101364522417153,
      "grad_norm": 15.149750709533691,
      "learning_rate": 3.131313131313132e-06,
      "loss": 0.1123,
      "step": 1690
    },
    {
      "epoch": 13.116959064327485,
      "grad_norm": 14.348451614379883,
      "learning_rate": 3.1111111111111116e-06,
      "loss": 0.1007,
      "step": 1692
    },
    {
      "epoch": 13.132553606237817,
      "grad_norm": 16.21964454650879,
      "learning_rate": 3.090909090909091e-06,
      "loss": 0.0714,
      "step": 1694
    },
    {
      "epoch": 13.148148148148149,
      "grad_norm": 16.18145179748535,
      "learning_rate": 3.0707070707070706e-06,
      "loss": 0.1161,
      "step": 1696
    },
    {
      "epoch": 13.163742690058479,
      "grad_norm": 12.515853881835938,
      "learning_rate": 3.0505050505050508e-06,
      "loss": 0.0764,
      "step": 1698
    },
    {
      "epoch": 13.17933723196881,
      "grad_norm": 15.352579116821289,
      "learning_rate": 3.0303030303030305e-06,
      "loss": 0.0669,
      "step": 1700
    },
    {
      "epoch": 13.194931773879143,
      "grad_norm": 15.448514938354492,
      "learning_rate": 3.0101010101010102e-06,
      "loss": 0.0632,
      "step": 1702
    },
    {
      "epoch": 13.210526315789474,
      "grad_norm": 8.946613311767578,
      "learning_rate": 2.98989898989899e-06,
      "loss": 0.1069,
      "step": 1704
    },
    {
      "epoch": 13.226120857699804,
      "grad_norm": 16.635780334472656,
      "learning_rate": 2.96969696969697e-06,
      "loss": 0.0732,
      "step": 1706
    },
    {
      "epoch": 13.241715399610136,
      "grad_norm": 13.56905746459961,
      "learning_rate": 2.94949494949495e-06,
      "loss": 0.0635,
      "step": 1708
    },
    {
      "epoch": 13.257309941520468,
      "grad_norm": 13.901637077331543,
      "learning_rate": 2.9292929292929295e-06,
      "loss": 0.0675,
      "step": 1710
    },
    {
      "epoch": 13.2729044834308,
      "grad_norm": 16.242685317993164,
      "learning_rate": 2.9090909090909093e-06,
      "loss": 0.0723,
      "step": 1712
    },
    {
      "epoch": 13.28849902534113,
      "grad_norm": 17.373977661132812,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.0672,
      "step": 1714
    },
    {
      "epoch": 13.304093567251462,
      "grad_norm": 14.994558334350586,
      "learning_rate": 2.868686868686869e-06,
      "loss": 0.1021,
      "step": 1716
    },
    {
      "epoch": 13.319688109161794,
      "grad_norm": 14.206889152526855,
      "learning_rate": 2.848484848484849e-06,
      "loss": 0.0736,
      "step": 1718
    },
    {
      "epoch": 13.335282651072125,
      "grad_norm": 13.715653419494629,
      "learning_rate": 2.8282828282828286e-06,
      "loss": 0.0661,
      "step": 1720
    },
    {
      "epoch": 13.350877192982455,
      "grad_norm": 19.77489471435547,
      "learning_rate": 2.808080808080808e-06,
      "loss": 0.1289,
      "step": 1722
    },
    {
      "epoch": 13.366471734892787,
      "grad_norm": 13.693623542785645,
      "learning_rate": 2.7878787878787885e-06,
      "loss": 0.0647,
      "step": 1724
    },
    {
      "epoch": 13.382066276803119,
      "grad_norm": 21.00936508178711,
      "learning_rate": 2.7676767676767678e-06,
      "loss": 0.0717,
      "step": 1726
    },
    {
      "epoch": 13.397660818713451,
      "grad_norm": 16.829511642456055,
      "learning_rate": 2.7474747474747475e-06,
      "loss": 0.0815,
      "step": 1728
    },
    {
      "epoch": 13.413255360623781,
      "grad_norm": 17.223661422729492,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 0.0679,
      "step": 1730
    },
    {
      "epoch": 13.428849902534113,
      "grad_norm": 11.047904968261719,
      "learning_rate": 2.7070707070707074e-06,
      "loss": 0.0755,
      "step": 1732
    },
    {
      "epoch": 13.444444444444445,
      "grad_norm": 10.979413986206055,
      "learning_rate": 2.686868686868687e-06,
      "loss": 0.0621,
      "step": 1734
    },
    {
      "epoch": 13.460038986354776,
      "grad_norm": 13.413798332214355,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.1106,
      "step": 1736
    },
    {
      "epoch": 13.475633528265107,
      "grad_norm": 25.85672378540039,
      "learning_rate": 2.6464646464646466e-06,
      "loss": 0.1015,
      "step": 1738
    },
    {
      "epoch": 13.491228070175438,
      "grad_norm": 17.88901710510254,
      "learning_rate": 2.6262626262626267e-06,
      "loss": 0.0648,
      "step": 1740
    },
    {
      "epoch": 13.50682261208577,
      "grad_norm": 10.817276954650879,
      "learning_rate": 2.6060606060606064e-06,
      "loss": 0.0778,
      "step": 1742
    },
    {
      "epoch": 13.522417153996102,
      "grad_norm": 13.71566390991211,
      "learning_rate": 2.585858585858586e-06,
      "loss": 0.0708,
      "step": 1744
    },
    {
      "epoch": 13.538011695906432,
      "grad_norm": 13.813069343566895,
      "learning_rate": 2.565656565656566e-06,
      "loss": 0.1069,
      "step": 1746
    },
    {
      "epoch": 13.553606237816764,
      "grad_norm": 19.229398727416992,
      "learning_rate": 2.5454545454545456e-06,
      "loss": 0.072,
      "step": 1748
    },
    {
      "epoch": 13.569200779727096,
      "grad_norm": 14.636473655700684,
      "learning_rate": 2.5252525252525258e-06,
      "loss": 0.0632,
      "step": 1750
    },
    {
      "epoch": 13.584795321637428,
      "grad_norm": 18.95419692993164,
      "learning_rate": 2.5050505050505055e-06,
      "loss": 0.0788,
      "step": 1752
    },
    {
      "epoch": 13.600389863547758,
      "grad_norm": 16.081924438476562,
      "learning_rate": 2.4848484848484848e-06,
      "loss": 0.0666,
      "step": 1754
    },
    {
      "epoch": 13.61598440545809,
      "grad_norm": 15.381420135498047,
      "learning_rate": 2.464646464646465e-06,
      "loss": 0.069,
      "step": 1756
    },
    {
      "epoch": 13.631578947368421,
      "grad_norm": 10.357014656066895,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 0.0615,
      "step": 1758
    },
    {
      "epoch": 13.647173489278753,
      "grad_norm": 16.975008010864258,
      "learning_rate": 2.4242424242424244e-06,
      "loss": 0.0735,
      "step": 1760
    },
    {
      "epoch": 13.662768031189083,
      "grad_norm": 15.641596794128418,
      "learning_rate": 2.404040404040404e-06,
      "loss": 0.067,
      "step": 1762
    },
    {
      "epoch": 13.678362573099415,
      "grad_norm": 16.192277908325195,
      "learning_rate": 2.3838383838383843e-06,
      "loss": 0.1034,
      "step": 1764
    },
    {
      "epoch": 13.693957115009747,
      "grad_norm": 12.52590560913086,
      "learning_rate": 2.363636363636364e-06,
      "loss": 0.0639,
      "step": 1766
    },
    {
      "epoch": 13.709551656920079,
      "grad_norm": 15.803915023803711,
      "learning_rate": 2.3434343434343437e-06,
      "loss": 0.0685,
      "step": 1768
    },
    {
      "epoch": 13.725146198830409,
      "grad_norm": 16.88402557373047,
      "learning_rate": 2.3232323232323234e-06,
      "loss": 0.143,
      "step": 1770
    },
    {
      "epoch": 13.74074074074074,
      "grad_norm": 18.621755599975586,
      "learning_rate": 2.303030303030303e-06,
      "loss": 0.088,
      "step": 1772
    },
    {
      "epoch": 13.756335282651072,
      "grad_norm": 15.756616592407227,
      "learning_rate": 2.282828282828283e-06,
      "loss": 0.0782,
      "step": 1774
    },
    {
      "epoch": 13.771929824561404,
      "grad_norm": 15.129638671875,
      "learning_rate": 2.262626262626263e-06,
      "loss": 0.1571,
      "step": 1776
    },
    {
      "epoch": 13.787524366471734,
      "grad_norm": 22.856822967529297,
      "learning_rate": 2.2424242424242428e-06,
      "loss": 0.0958,
      "step": 1778
    },
    {
      "epoch": 13.803118908382066,
      "grad_norm": 22.457216262817383,
      "learning_rate": 2.222222222222222e-06,
      "loss": 0.07,
      "step": 1780
    },
    {
      "epoch": 13.818713450292398,
      "grad_norm": 22.231103897094727,
      "learning_rate": 2.2020202020202022e-06,
      "loss": 0.1307,
      "step": 1782
    },
    {
      "epoch": 13.83430799220273,
      "grad_norm": 14.10673999786377,
      "learning_rate": 2.181818181818182e-06,
      "loss": 0.0816,
      "step": 1784
    },
    {
      "epoch": 13.84990253411306,
      "grad_norm": 21.09175682067871,
      "learning_rate": 2.1616161616161617e-06,
      "loss": 0.0703,
      "step": 1786
    },
    {
      "epoch": 13.865497076023392,
      "grad_norm": 19.931779861450195,
      "learning_rate": 2.1414141414141414e-06,
      "loss": 0.0851,
      "step": 1788
    },
    {
      "epoch": 13.881091617933723,
      "grad_norm": 13.039023399353027,
      "learning_rate": 2.1212121212121216e-06,
      "loss": 0.0607,
      "step": 1790
    },
    {
      "epoch": 13.896686159844055,
      "grad_norm": 15.205140113830566,
      "learning_rate": 2.1010101010101013e-06,
      "loss": 0.0731,
      "step": 1792
    },
    {
      "epoch": 13.912280701754385,
      "grad_norm": 15.254611015319824,
      "learning_rate": 2.080808080808081e-06,
      "loss": 0.0664,
      "step": 1794
    },
    {
      "epoch": 13.927875243664717,
      "grad_norm": 15.84323787689209,
      "learning_rate": 2.0606060606060607e-06,
      "loss": 0.1323,
      "step": 1796
    },
    {
      "epoch": 13.943469785575049,
      "grad_norm": 18.722816467285156,
      "learning_rate": 2.0404040404040405e-06,
      "loss": 0.083,
      "step": 1798
    },
    {
      "epoch": 13.95906432748538,
      "grad_norm": 13.077245712280273,
      "learning_rate": 2.02020202020202e-06,
      "loss": 0.0674,
      "step": 1800
    },
    {
      "epoch": 13.97465886939571,
      "grad_norm": 15.237214088439941,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0618,
      "step": 1802
    },
    {
      "epoch": 13.990253411306043,
      "grad_norm": 15.363524436950684,
      "learning_rate": 1.97979797979798e-06,
      "loss": 0.063,
      "step": 1804
    },
    {
      "epoch": 14.0,
      "grad_norm": 6.827075004577637,
      "learning_rate": 1.9595959595959598e-06,
      "loss": 0.0611,
      "step": 1806
    },
    {
      "epoch": 14.015594541910332,
      "grad_norm": 11.27649974822998,
      "learning_rate": 1.9393939393939395e-06,
      "loss": 0.0584,
      "step": 1808
    },
    {
      "epoch": 14.031189083820664,
      "grad_norm": 17.313072204589844,
      "learning_rate": 1.9191919191919192e-06,
      "loss": 0.0579,
      "step": 1810
    },
    {
      "epoch": 14.046783625730994,
      "grad_norm": 8.910266876220703,
      "learning_rate": 1.8989898989898992e-06,
      "loss": 0.0567,
      "step": 1812
    },
    {
      "epoch": 14.062378167641326,
      "grad_norm": 9.767702102661133,
      "learning_rate": 1.878787878787879e-06,
      "loss": 0.0513,
      "step": 1814
    },
    {
      "epoch": 14.077972709551657,
      "grad_norm": 15.328595161437988,
      "learning_rate": 1.8585858585858588e-06,
      "loss": 0.0895,
      "step": 1816
    },
    {
      "epoch": 14.093567251461987,
      "grad_norm": 15.058792114257812,
      "learning_rate": 1.8383838383838384e-06,
      "loss": 0.1056,
      "step": 1818
    },
    {
      "epoch": 14.10916179337232,
      "grad_norm": 9.884140014648438,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 0.0666,
      "step": 1820
    },
    {
      "epoch": 14.124756335282651,
      "grad_norm": 17.07447052001953,
      "learning_rate": 1.797979797979798e-06,
      "loss": 0.0622,
      "step": 1822
    },
    {
      "epoch": 14.140350877192983,
      "grad_norm": 15.92841911315918,
      "learning_rate": 1.777777777777778e-06,
      "loss": 0.0912,
      "step": 1824
    },
    {
      "epoch": 14.155945419103315,
      "grad_norm": 17.04329490661621,
      "learning_rate": 1.7575757575757577e-06,
      "loss": 0.1168,
      "step": 1826
    },
    {
      "epoch": 14.171539961013645,
      "grad_norm": 14.932868003845215,
      "learning_rate": 1.7373737373737376e-06,
      "loss": 0.062,
      "step": 1828
    },
    {
      "epoch": 14.187134502923977,
      "grad_norm": 14.902604103088379,
      "learning_rate": 1.7171717171717173e-06,
      "loss": 0.0685,
      "step": 1830
    },
    {
      "epoch": 14.202729044834308,
      "grad_norm": 9.769229888916016,
      "learning_rate": 1.6969696969696973e-06,
      "loss": 0.0528,
      "step": 1832
    },
    {
      "epoch": 14.218323586744638,
      "grad_norm": 12.67512035369873,
      "learning_rate": 1.6767676767676768e-06,
      "loss": 0.0642,
      "step": 1834
    },
    {
      "epoch": 14.23391812865497,
      "grad_norm": 16.457324981689453,
      "learning_rate": 1.6565656565656567e-06,
      "loss": 0.0785,
      "step": 1836
    },
    {
      "epoch": 14.249512670565302,
      "grad_norm": 18.60617446899414,
      "learning_rate": 1.6363636363636365e-06,
      "loss": 0.077,
      "step": 1838
    },
    {
      "epoch": 14.265107212475634,
      "grad_norm": 12.149162292480469,
      "learning_rate": 1.6161616161616164e-06,
      "loss": 0.0617,
      "step": 1840
    },
    {
      "epoch": 14.280701754385966,
      "grad_norm": 16.446054458618164,
      "learning_rate": 1.5959595959595961e-06,
      "loss": 0.0642,
      "step": 1842
    },
    {
      "epoch": 14.296296296296296,
      "grad_norm": 12.38260269165039,
      "learning_rate": 1.5757575757575759e-06,
      "loss": 0.0736,
      "step": 1844
    },
    {
      "epoch": 14.311890838206628,
      "grad_norm": 12.871152877807617,
      "learning_rate": 1.5555555555555558e-06,
      "loss": 0.0718,
      "step": 1846
    },
    {
      "epoch": 14.32748538011696,
      "grad_norm": 16.69586181640625,
      "learning_rate": 1.5353535353535353e-06,
      "loss": 0.0718,
      "step": 1848
    },
    {
      "epoch": 14.34307992202729,
      "grad_norm": 13.943997383117676,
      "learning_rate": 1.5151515151515152e-06,
      "loss": 0.0628,
      "step": 1850
    },
    {
      "epoch": 14.358674463937621,
      "grad_norm": 11.280786514282227,
      "learning_rate": 1.494949494949495e-06,
      "loss": 0.0652,
      "step": 1852
    },
    {
      "epoch": 14.374269005847953,
      "grad_norm": 13.937952041625977,
      "learning_rate": 1.474747474747475e-06,
      "loss": 0.0543,
      "step": 1854
    },
    {
      "epoch": 14.389863547758285,
      "grad_norm": 17.335994720458984,
      "learning_rate": 1.4545454545454546e-06,
      "loss": 0.1039,
      "step": 1856
    },
    {
      "epoch": 14.405458089668617,
      "grad_norm": 16.474071502685547,
      "learning_rate": 1.4343434343434346e-06,
      "loss": 0.0699,
      "step": 1858
    },
    {
      "epoch": 14.421052631578947,
      "grad_norm": 15.907944679260254,
      "learning_rate": 1.4141414141414143e-06,
      "loss": 0.0727,
      "step": 1860
    },
    {
      "epoch": 14.436647173489279,
      "grad_norm": 15.123119354248047,
      "learning_rate": 1.3939393939393942e-06,
      "loss": 0.0679,
      "step": 1862
    },
    {
      "epoch": 14.45224171539961,
      "grad_norm": 16.88827896118164,
      "learning_rate": 1.3737373737373738e-06,
      "loss": 0.0698,
      "step": 1864
    },
    {
      "epoch": 14.46783625730994,
      "grad_norm": 13.288071632385254,
      "learning_rate": 1.3535353535353537e-06,
      "loss": 0.0644,
      "step": 1866
    },
    {
      "epoch": 14.483430799220272,
      "grad_norm": 16.054841995239258,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0695,
      "step": 1868
    },
    {
      "epoch": 14.499025341130604,
      "grad_norm": 10.713949203491211,
      "learning_rate": 1.3131313131313134e-06,
      "loss": 0.063,
      "step": 1870
    },
    {
      "epoch": 14.514619883040936,
      "grad_norm": 12.700723648071289,
      "learning_rate": 1.292929292929293e-06,
      "loss": 0.0624,
      "step": 1872
    },
    {
      "epoch": 14.530214424951268,
      "grad_norm": 11.77884292602539,
      "learning_rate": 1.2727272727272728e-06,
      "loss": 0.054,
      "step": 1874
    },
    {
      "epoch": 14.545808966861598,
      "grad_norm": 12.288275718688965,
      "learning_rate": 1.2525252525252527e-06,
      "loss": 0.0557,
      "step": 1876
    },
    {
      "epoch": 14.56140350877193,
      "grad_norm": 16.760772705078125,
      "learning_rate": 1.2323232323232325e-06,
      "loss": 0.1156,
      "step": 1878
    },
    {
      "epoch": 14.576998050682262,
      "grad_norm": 18.376953125,
      "learning_rate": 1.2121212121212122e-06,
      "loss": 0.1008,
      "step": 1880
    },
    {
      "epoch": 14.592592592592592,
      "grad_norm": 14.565071105957031,
      "learning_rate": 1.1919191919191921e-06,
      "loss": 0.0762,
      "step": 1882
    },
    {
      "epoch": 14.608187134502923,
      "grad_norm": 17.001935958862305,
      "learning_rate": 1.1717171717171719e-06,
      "loss": 0.0873,
      "step": 1884
    },
    {
      "epoch": 14.623781676413255,
      "grad_norm": 17.386804580688477,
      "learning_rate": 1.1515151515151516e-06,
      "loss": 0.0674,
      "step": 1886
    },
    {
      "epoch": 14.639376218323587,
      "grad_norm": 18.594648361206055,
      "learning_rate": 1.1313131313131315e-06,
      "loss": 0.0992,
      "step": 1888
    },
    {
      "epoch": 14.654970760233919,
      "grad_norm": 17.022937774658203,
      "learning_rate": 1.111111111111111e-06,
      "loss": 0.1133,
      "step": 1890
    },
    {
      "epoch": 14.670565302144249,
      "grad_norm": 14.92554759979248,
      "learning_rate": 1.090909090909091e-06,
      "loss": 0.1009,
      "step": 1892
    },
    {
      "epoch": 14.68615984405458,
      "grad_norm": 13.3340425491333,
      "learning_rate": 1.0707070707070707e-06,
      "loss": 0.0682,
      "step": 1894
    },
    {
      "epoch": 14.701754385964913,
      "grad_norm": 16.603878021240234,
      "learning_rate": 1.0505050505050506e-06,
      "loss": 0.0958,
      "step": 1896
    },
    {
      "epoch": 14.717348927875243,
      "grad_norm": 13.287442207336426,
      "learning_rate": 1.0303030303030304e-06,
      "loss": 0.0603,
      "step": 1898
    },
    {
      "epoch": 14.732943469785575,
      "grad_norm": 18.366472244262695,
      "learning_rate": 1.01010101010101e-06,
      "loss": 0.0817,
      "step": 1900
    },
    {
      "epoch": 14.748538011695906,
      "grad_norm": 17.70624542236328,
      "learning_rate": 9.8989898989899e-07,
      "loss": 0.0849,
      "step": 1902
    },
    {
      "epoch": 14.764132553606238,
      "grad_norm": 16.861135482788086,
      "learning_rate": 9.696969696969698e-07,
      "loss": 0.0712,
      "step": 1904
    },
    {
      "epoch": 14.77972709551657,
      "grad_norm": 10.539166450500488,
      "learning_rate": 9.494949494949496e-07,
      "loss": 0.0813,
      "step": 1906
    },
    {
      "epoch": 14.7953216374269,
      "grad_norm": 14.38422966003418,
      "learning_rate": 9.292929292929294e-07,
      "loss": 0.0623,
      "step": 1908
    },
    {
      "epoch": 14.810916179337232,
      "grad_norm": 17.474449157714844,
      "learning_rate": 9.090909090909091e-07,
      "loss": 0.0886,
      "step": 1910
    },
    {
      "epoch": 14.826510721247564,
      "grad_norm": 14.13970947265625,
      "learning_rate": 8.88888888888889e-07,
      "loss": 0.0739,
      "step": 1912
    },
    {
      "epoch": 14.842105263157894,
      "grad_norm": 16.98288917541504,
      "learning_rate": 8.686868686868688e-07,
      "loss": 0.0753,
      "step": 1914
    },
    {
      "epoch": 14.857699805068226,
      "grad_norm": 19.39560317993164,
      "learning_rate": 8.484848484848486e-07,
      "loss": 0.0675,
      "step": 1916
    },
    {
      "epoch": 14.873294346978557,
      "grad_norm": 15.391788482666016,
      "learning_rate": 8.282828282828284e-07,
      "loss": 0.1044,
      "step": 1918
    },
    {
      "epoch": 14.88888888888889,
      "grad_norm": 14.6919527053833,
      "learning_rate": 8.080808080808082e-07,
      "loss": 0.0913,
      "step": 1920
    },
    {
      "epoch": 14.904483430799221,
      "grad_norm": 10.746387481689453,
      "learning_rate": 7.878787878787879e-07,
      "loss": 0.0724,
      "step": 1922
    },
    {
      "epoch": 14.920077972709551,
      "grad_norm": 12.202763557434082,
      "learning_rate": 7.676767676767677e-07,
      "loss": 0.0851,
      "step": 1924
    },
    {
      "epoch": 14.935672514619883,
      "grad_norm": 13.62994384765625,
      "learning_rate": 7.474747474747475e-07,
      "loss": 0.0697,
      "step": 1926
    },
    {
      "epoch": 14.951267056530215,
      "grad_norm": 11.414361000061035,
      "learning_rate": 7.272727272727273e-07,
      "loss": 0.0567,
      "step": 1928
    },
    {
      "epoch": 14.966861598440545,
      "grad_norm": 17.4656925201416,
      "learning_rate": 7.070707070707071e-07,
      "loss": 0.1151,
      "step": 1930
    },
    {
      "epoch": 14.982456140350877,
      "grad_norm": 18.94188117980957,
      "learning_rate": 6.868686868686869e-07,
      "loss": 0.0665,
      "step": 1932
    },
    {
      "epoch": 14.998050682261209,
      "grad_norm": 19.040395736694336,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0654,
      "step": 1934
    },
    {
      "epoch": 15.007797270955166,
      "grad_norm": 12.258501052856445,
      "learning_rate": 6.464646464646465e-07,
      "loss": 0.0362,
      "step": 1936
    },
    {
      "epoch": 15.023391812865498,
      "grad_norm": 14.61597728729248,
      "learning_rate": 6.262626262626264e-07,
      "loss": 0.0556,
      "step": 1938
    },
    {
      "epoch": 15.038986354775828,
      "grad_norm": 15.973652839660645,
      "learning_rate": 6.060606060606061e-07,
      "loss": 0.1171,
      "step": 1940
    },
    {
      "epoch": 15.05458089668616,
      "grad_norm": 10.473252296447754,
      "learning_rate": 5.858585858585859e-07,
      "loss": 0.0548,
      "step": 1942
    },
    {
      "epoch": 15.070175438596491,
      "grad_norm": 14.85037899017334,
      "learning_rate": 5.656565656565658e-07,
      "loss": 0.0858,
      "step": 1944
    },
    {
      "epoch": 15.085769980506823,
      "grad_norm": 13.12698745727539,
      "learning_rate": 5.454545454545455e-07,
      "loss": 0.1061,
      "step": 1946
    },
    {
      "epoch": 15.101364522417153,
      "grad_norm": 12.77132511138916,
      "learning_rate": 5.252525252525253e-07,
      "loss": 0.1002,
      "step": 1948
    },
    {
      "epoch": 15.116959064327485,
      "grad_norm": 11.830368995666504,
      "learning_rate": 5.05050505050505e-07,
      "loss": 0.0724,
      "step": 1950
    },
    {
      "epoch": 15.132553606237817,
      "grad_norm": 11.412312507629395,
      "learning_rate": 4.848484848484849e-07,
      "loss": 0.0509,
      "step": 1952
    },
    {
      "epoch": 15.148148148148149,
      "grad_norm": 12.896754264831543,
      "learning_rate": 4.646464646464647e-07,
      "loss": 0.0589,
      "step": 1954
    },
    {
      "epoch": 15.163742690058479,
      "grad_norm": 10.9576997756958,
      "learning_rate": 4.444444444444445e-07,
      "loss": 0.0635,
      "step": 1956
    },
    {
      "epoch": 15.17933723196881,
      "grad_norm": 11.683606147766113,
      "learning_rate": 4.242424242424243e-07,
      "loss": 0.0938,
      "step": 1958
    },
    {
      "epoch": 15.194931773879143,
      "grad_norm": 16.96512794494629,
      "learning_rate": 4.040404040404041e-07,
      "loss": 0.0703,
      "step": 1960
    },
    {
      "epoch": 15.210526315789474,
      "grad_norm": 13.937493324279785,
      "learning_rate": 3.838383838383838e-07,
      "loss": 0.0831,
      "step": 1962
    },
    {
      "epoch": 15.226120857699804,
      "grad_norm": 11.6497802734375,
      "learning_rate": 3.6363636363636366e-07,
      "loss": 0.0597,
      "step": 1964
    },
    {
      "epoch": 15.241715399610136,
      "grad_norm": 15.599076271057129,
      "learning_rate": 3.4343434343434344e-07,
      "loss": 0.0634,
      "step": 1966
    },
    {
      "epoch": 15.257309941520468,
      "grad_norm": 12.674434661865234,
      "learning_rate": 3.2323232323232327e-07,
      "loss": 0.0547,
      "step": 1968
    },
    {
      "epoch": 15.2729044834308,
      "grad_norm": 13.563188552856445,
      "learning_rate": 3.0303030303030305e-07,
      "loss": 0.059,
      "step": 1970
    },
    {
      "epoch": 15.28849902534113,
      "grad_norm": 17.74893569946289,
      "learning_rate": 2.828282828282829e-07,
      "loss": 0.0856,
      "step": 1972
    },
    {
      "epoch": 15.304093567251462,
      "grad_norm": 17.18756103515625,
      "learning_rate": 2.6262626262626266e-07,
      "loss": 0.0921,
      "step": 1974
    },
    {
      "epoch": 15.319688109161794,
      "grad_norm": 15.784802436828613,
      "learning_rate": 2.4242424242424244e-07,
      "loss": 0.0755,
      "step": 1976
    },
    {
      "epoch": 15.335282651072125,
      "grad_norm": 10.375271797180176,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 0.0546,
      "step": 1978
    },
    {
      "epoch": 15.350877192982455,
      "grad_norm": 13.623123168945312,
      "learning_rate": 2.0202020202020205e-07,
      "loss": 0.0614,
      "step": 1980
    },
    {
      "epoch": 15.366471734892787,
      "grad_norm": 12.762528419494629,
      "learning_rate": 1.8181818181818183e-07,
      "loss": 0.0545,
      "step": 1982
    },
    {
      "epoch": 15.382066276803119,
      "grad_norm": 14.428118705749512,
      "learning_rate": 1.6161616161616163e-07,
      "loss": 0.0822,
      "step": 1984
    },
    {
      "epoch": 15.397660818713451,
      "grad_norm": 12.454866409301758,
      "learning_rate": 1.4141414141414144e-07,
      "loss": 0.0593,
      "step": 1986
    },
    {
      "epoch": 15.413255360623781,
      "grad_norm": 15.737441062927246,
      "learning_rate": 1.2121212121212122e-07,
      "loss": 0.0628,
      "step": 1988
    },
    {
      "epoch": 15.428849902534113,
      "grad_norm": 12.912419319152832,
      "learning_rate": 1.0101010101010103e-07,
      "loss": 0.0511,
      "step": 1990
    },
    {
      "epoch": 15.444444444444445,
      "grad_norm": 13.672311782836914,
      "learning_rate": 8.080808080808082e-08,
      "loss": 0.0609,
      "step": 1992
    },
    {
      "epoch": 15.460038986354776,
      "grad_norm": 10.126008987426758,
      "learning_rate": 6.060606060606061e-08,
      "loss": 0.0631,
      "step": 1994
    },
    {
      "epoch": 15.475633528265107,
      "grad_norm": 12.561980247497559,
      "learning_rate": 4.040404040404041e-08,
      "loss": 0.0638,
      "step": 1996
    },
    {
      "epoch": 15.491228070175438,
      "grad_norm": 13.746444702148438,
      "learning_rate": 2.0202020202020204e-08,
      "loss": 0.0831,
      "step": 1998
    },
    {
      "epoch": 15.50682261208577,
      "grad_norm": 12.254122734069824,
      "learning_rate": 0.0,
      "loss": 0.062,
      "step": 2000
    }
  ],
  "logging_steps": 2,
  "max_steps": 2000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 16,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.27916701732352e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
